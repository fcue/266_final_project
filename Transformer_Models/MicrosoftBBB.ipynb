{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEURsjghWMCJ",
        "outputId": "dc94da79-390c-408f-bafa-da2e5ce4de5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DixhWmqWcy3M"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYBZ86dRc2bq",
        "outputId": "0526df96-0a4d-4ac5-c367-701185d9a80a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPL27mL0_sTC",
        "outputId": "5b20d573-e9c3-4813-bc69-a5c4632e9f57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# verify GPU\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyMy7PgZ_mME"
      },
      "source": [
        "## Convert to DataFrame for EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJ6HnnrvKF_q"
      },
      "outputs": [],
      "source": [
        "def convert_data(filepath):\n",
        "\n",
        "  # Read the data from the text file\n",
        "  with open(filepath, \"r\") as file:\n",
        "      lines = file.readlines()\n",
        "\n",
        "  # Define an empty list to store the data\n",
        "  data = []\n",
        "\n",
        "  # Iterate over each line in the file\n",
        "  for line in lines:\n",
        "      # Split the line by spaces\n",
        "      parts = line.strip().split()\n",
        "\n",
        "      # Check if the line has the expected number of elements\n",
        "      if len(parts) == 9:\n",
        "          # Extract the values from the line\n",
        "          text_file_name = parts[0]\n",
        "          sentence_line_number = int(parts[1])\n",
        "          sentence_word_index = int(parts[2])\n",
        "          sentence_seq = parts[3]\n",
        "          start_token = int(parts[4])\n",
        "          end_token = int(parts[5])\n",
        "          original_word = parts[6]\n",
        "          word = parts[7]\n",
        "          label = parts[8]\n",
        "\n",
        "          # Append the values as a tuple to the data list\n",
        "          data.append((text_file_name, sentence_line_number, sentence_word_index, sentence_seq,\n",
        "                      start_token, end_token, original_word, word, label))\n",
        "\n",
        "  # Create a DataFrame from the data list with appropriate column names\n",
        "  df = pd.DataFrame(data, columns=['text_file_name', 'sentence_line_number', 'sentence_word_index',\n",
        "                                  'sentence_seq', 'start_token', 'end_token', 'original_word',\n",
        "                                  'word', 'label'])\n",
        "\n",
        "  return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBwbw0ddKYcJ",
        "outputId": "97d2a566-c640-42df-a81b-8a9cb402c779"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of train: 895141\n",
            "Length of test: 585761\n"
          ]
        }
      ],
      "source": [
        "train_data_path = \"/content/drive/MyDrive/266_final/data/Original_text/dataset1_train.txt\"\n",
        "test_data_path = \"/content/drive/MyDrive/266_final/data/Original_text/dataset1_test.txt\"\n",
        "\n",
        "train = convert_data(train_data_path)\n",
        "test = convert_data(test_data_path)\n",
        "\n",
        "print(f\"Length of train: {len(train)}\")\n",
        "print(f\"Length of test: {len(test)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqnvelmiGLXi",
        "outputId": "b315cf24-9160-454c-a2d8-c87b331e62ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1053"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "df = train.copy()\n",
        "df['sentence_line_number'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVvq1qgWw1vR",
        "outputId": "bdd2bf71-3e09-4297-d1a9-cd587a06a961"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "I-Route           397\n",
              "B-Duration        592\n",
              "I-ADE             776\n",
              "B-ADE             956\n",
              "I-Duration       1034\n",
              "I-Reason         3125\n",
              "B-Reason         3791\n",
              "I-Form           4173\n",
              "B-Dosage         4221\n",
              "I-Drug           4298\n",
              "B-Route          5475\n",
              "B-Frequency      6279\n",
              "I-Strength       6617\n",
              "B-Form           6647\n",
              "B-Strength       6691\n",
              "I-Dosage         8779\n",
              "I-Frequency     13023\n",
              "B-Drug          16222\n",
              "O              802045\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "df['label'].value_counts().sort_values()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWzDzNtXJRSa"
      },
      "source": [
        "### Label labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXnnrZqSIX95",
        "outputId": "9e85d2cd-9287-4f27-ed66-e4b754d2134d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'B-Route', 'I-Drug', 'I-ADE', 'I-Strength', 'B-Duration', 'B-Frequency', 'B-Reason', 'I-Reason', 'B-Dosage', 'I-Form', 'B-Form', 'B-Strength', 'I-Dosage', 'I-Duration', 'I-Route', 'O', 'B-Drug', 'B-ADE', 'I-Frequency'}\n"
          ]
        }
      ],
      "source": [
        "# Split labels based on whitespace and turn them into a list\n",
        "labels = [i.split() for i in df['label'].values.tolist()]\n",
        "\n",
        "# Check how many labels are there in the dataset\n",
        "unique_labels = set()\n",
        "\n",
        "for lb in labels:\n",
        "  [unique_labels.add(i) for i in lb if i not in unique_labels]\n",
        "\n",
        "print(unique_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ux83z2FGLU05",
        "outputId": "d6a31096-297f-42fe-ad50-74e8d7002134"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'B-ADE': 0, 'B-Dosage': 1, 'B-Drug': 2, 'B-Duration': 3, 'B-Form': 4, 'B-Frequency': 5, 'B-Reason': 6, 'B-Route': 7, 'B-Strength': 8, 'I-ADE': 9, 'I-Dosage': 10, 'I-Drug': 11, 'I-Duration': 12, 'I-Form': 13, 'I-Frequency': 14, 'I-Reason': 15, 'I-Route': 16, 'I-Strength': 17, 'O': 18}\n"
          ]
        }
      ],
      "source": [
        "# Map each label into its id representation and vice versa\n",
        "labels_to_ids = {k: v for v, k in enumerate(sorted(unique_labels))}\n",
        "ids_to_labels = {v: k for v, k in enumerate(sorted(unique_labels))}\n",
        "print(labels_to_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4ukPCa0-SG6"
      },
      "outputs": [],
      "source": [
        "def formatted_df(df):\n",
        "  df['sentence'] = df[[\n",
        "      'text_file_name',\n",
        "      'sentence_line_number',\n",
        "      'original_word',\n",
        "      'label']].groupby(\n",
        "          ['text_file_name', 'sentence_line_number'])['original_word'].transform(lambda x: ' '.join(x))\n",
        "\n",
        "  df['word_labels'] = df[[\n",
        "      'text_file_name',\n",
        "      'sentence_line_number',\n",
        "      'original_word',\n",
        "      'label']].groupby(\n",
        "          ['text_file_name', 'sentence_line_number'])['label'].transform(lambda x: ','.join(x))\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KfHxKgI-d7M"
      },
      "outputs": [],
      "source": [
        "df = formatted_df(df)\n",
        "df_test = formatted_df(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxe_t7P5Ae-V"
      },
      "outputs": [],
      "source": [
        "def sentence_level_data_fn(df):\n",
        "  sentence_level_data = df[[\"text_file_name\", \"sentence_line_number\", \"sentence\", \"word_labels\"]].drop_duplicates().reset_index(drop=True)\n",
        "  return sentence_level_data\n",
        "\n",
        "sentence_level_train = sentence_level_data_fn(df)\n",
        "sentence_level_test = sentence_level_data_fn(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpGwter59c47"
      },
      "outputs": [],
      "source": [
        "sentence_level_data = sentence_level_train[[\"text_file_name\", \"sentence_line_number\", \"sentence\", \"word_labels\"]].drop_duplicates().reset_index(drop=True)\n",
        "sentence_level_test = sentence_level_test[[\"text_file_name\",\"sentence_line_number\",  \"sentence\", \"word_labels\"]].drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "sentence_level_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1UvdlPAM67e"
      },
      "source": [
        "#### Some sentences are really short. *Try* concatenating sentence to get longer inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMGe3A5OJ_Pb"
      },
      "outputs": [],
      "source": [
        "df = sentence_level_data.sort_values(by=['text_file_name', 'sentence_line_number'])\n",
        "df_test = sentence_level_test.sort_values(by=['text_file_name', 'sentence_line_number'])\n",
        "\n",
        "def cs(dataframe):\n",
        "    processed_data_list = []\n",
        "\n",
        "    # Group by 'text_file_name'\n",
        "    grouped = dataframe.groupby(\"text_file_name\")\n",
        "\n",
        "    for name, group in grouped:\n",
        "        sentences = group['sentence'].tolist()\n",
        "        # Labels are already strings, so we take them as is\n",
        "        labels = group['word_labels'].tolist()\n",
        "\n",
        "        for i in range(0, len(sentences), 5):\n",
        "            end_index = min(i + 5, len(sentences))\n",
        "            current_batch = sentences[i:end_index]\n",
        "            current_labels = labels[i:end_index]\n",
        "\n",
        "            concatenated_sentence = \" \".join(current_batch)\n",
        "            # Concatenate labels as they are, assuming they're correctly formatted strings\n",
        "            concatenated_label = \",\".join(current_labels)\n",
        "\n",
        "            processed_data_list.append({\n",
        "                \"text_file_name\": name,\n",
        "                \"sentences\": concatenated_sentence,\n",
        "                \"labels\": concatenated_label\n",
        "            })\n",
        "\n",
        "    processed_data = pd.DataFrame(processed_data_list)\n",
        "    return processed_data\n",
        "\n",
        "\n",
        "# Correcting the function to properly handle labels\n",
        "concatenated_df = cs(df)\n",
        "concatenated_test= cs(df_test)\n",
        "\n",
        "concatenated_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "Navmg0_pEA-q",
        "outputId": "eb1e3aab-3bc7-430a-a564-52b04930643a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"could be due to patient's liver dysfunction/third spacing from CHF . If cholecystitis is of clinical concern , HIDA scan can be performed provided the total bilirubin is not elevated . 3 ) Hyperdense renal cortex in left lower quadrant transplanted kidney . Findings are most likely due to chronic rejection or\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "# make sure test is formatted same way\n",
        "concatenated_df.iloc[301].sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9a4wW-yFPLAo"
      },
      "outputs": [],
      "source": [
        "def contains_ade(labels):\n",
        "    return any('ADE' in label for label in labels.split(','))\n",
        "\n",
        "ade_mask = concatenated_df['labels'].apply(contains_ade)\n",
        "\n",
        "# Use the mask to filter the DataFrame\n",
        "ade_sentences_df = concatenated_df[ade_mask]\n",
        "\n",
        "# Display the filtered DataFrame\n",
        "ade_sentences_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3En5RdvFr_O"
      },
      "outputs": [],
      "source": [
        "data = concatenated_df.copy()\n",
        "data_test = concatenated_test.copy()\n",
        "\n",
        "data.rename(columns={'concatenated_sentence': 'sentence', 'concatenated_labels': 'word_labels'}, inplace=True)\n",
        "data_test.rename(columns={'concatenated_sentence': 'sentence', 'concatenated_labels': 'word_labels'}, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIRanj57PpZu"
      },
      "source": [
        "### It looks reasonable, let's proceed with training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\",\n",
        "    num_labels=len(labels_to_ids))"
      ],
      "metadata": {
        "id": "fINdQRqymGY0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a89e2116-66bb-4fae-86e1-293b38a68670"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRjrBQAR9-zc"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 512\n",
        "TRAIN_BATCH_SIZE = 4\n",
        "VALID_BATCH_SIZE = 2\n",
        "TEST_BATCH_SIZE = 32\n",
        "EPOCHS = 5 # train for 5\n",
        "LEARNING_RATE = 1e-05\n",
        "MAX_GRAD_NORM = 10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYVPN8o4ve9Q"
      },
      "outputs": [],
      "source": [
        "class dataset(Dataset):\n",
        "  def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        # step 1: get the sentence and word labels\n",
        "        sentence = self.data.sentences[index].strip().split()\n",
        "        word_labels = self.data.labels[index].split(\",\")\n",
        "        # sentence_id = self.data.iloc[index]['sentence_id']\n",
        "\n",
        "        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n",
        "        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n",
        "        encoding = self.tokenizer(sentence,\n",
        "                             return_offsets_mapping=True,\n",
        "                             padding='max_length',\n",
        "                             is_split_into_words=True,\n",
        "                             truncation=True,\n",
        "                             max_length=self.max_len)\n",
        "\n",
        "        # step 3: create token labels only for first word pieces of each tokenized word\n",
        "        labels = [labels_to_ids[label] for label in word_labels]\n",
        "        # code based on https://huggingface.co/transformers/custom_datasets.html#tok-ner\n",
        "        # create an empty array of -100 of length max_length\n",
        "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
        "\n",
        "        # set only labels whose first offset position is 0 and the second is not 0\n",
        "        i = 0\n",
        "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
        "          if mapping[0] == 0 and mapping[1] != 0:\n",
        "            # overwrite label\n",
        "            encoded_labels[idx] = labels[i]\n",
        "            i += 1\n",
        "\n",
        "        # step 4: turn everything into PyTorch tensors\n",
        "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
        "        item['labels'] = torch.as_tensor(encoded_labels)\n",
        "        # item['sentence_id'] = sentence_id\n",
        "\n",
        "        return item\n",
        "\n",
        "  def __len__(self):\n",
        "        return self.len"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_dataset, val_dataset = train_test_split(data, test_size=0.2, shuffle=False)\n"
      ],
      "metadata": {
        "id": "C9w4vm1oAZ4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.reset_index(drop=True, inplace=True)\n",
        "val_dataset.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "6cwkfORqBDMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset['sent_id']  = train_dataset.index\n",
        "val_dataset['sent_id']= val_dataset.index\n",
        "data_test['sent_id'] = data_test.index\n"
      ],
      "metadata": {
        "id": "eabNjnwEAef5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDFvMkzgQjUX",
        "outputId": "337129f8-c1e1-4f5d-a673-fdc84038dfde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FULL Dataet: (16788, 3)\n",
            "TRAIN Dataset: (13430, 4)\n",
            "VAL Dataset: (3358, 4)\n",
            "TEST Dataset: (11146, 4)\n"
          ]
        }
      ],
      "source": [
        "print(\"FULL Dataet: {}\".format(data.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"VAL Dataset: {}\".format(val_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(data_test.shape))\n",
        "\n",
        "data_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
        "val_set = dataset(val_dataset, tokenizer, MAX_LEN)\n",
        "test_set = dataset(data_test, tokenizer, MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4v9CdAR3b9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ec60f68-4fc7-4f3c-9b7a-fb8c9a780895"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS]       -100\n",
            "to          18\n",
            "an          18\n",
            "os          18\n",
            "##h         -100\n",
            "with        18\n",
            "dyspnea     18\n",
            "now         18\n",
            "admitted    18\n",
            "to          18\n",
            "the         18\n",
            "mic         18\n",
            "##u         -100\n",
            "after       18\n",
            "pea         18\n",
            "arrest      18\n",
            "x2          18\n",
            ".           18\n",
            "the         18\n",
            "patient     18\n",
            "initially   18\n",
            "presented   18\n",
            "to          18\n",
            "lg          18\n",
            "##h         -100\n",
            "ed          18\n",
            "with        18\n",
            "hypox       18\n",
            "##emic      -100\n",
            "respiratory  18\n",
            "distress    18\n",
            ".           18\n",
            "while       18\n",
            "at          18\n",
            "the         18\n",
            "os          18\n",
            "##h         -100\n",
            ",           18\n",
            "he          18\n",
            "received    18\n",
            "ctx         2\n",
            ",           18\n",
            "azithromycin  2\n",
            ",           18\n",
            "sc          7\n",
            "epinephrine  2\n",
            ",           18\n",
            "and         18\n",
            "sol         2\n",
            "##ume       -100\n",
            "##dr        -100\n",
            "##ol        -100\n",
            ".           18\n",
            "while       18\n",
            "at          18\n",
            "the         18\n",
            "os          18\n",
            "##h         -100\n",
            ",           18\n",
            "he          18\n",
            "became      18\n",
            "confused    18\n",
            "and         18\n",
            "subsequently  18\n",
            "had         18\n",
            "an          18\n",
            "episode     18\n",
            "of          18\n",
            "pea         18\n",
            "arrest      18\n",
            "[SEP]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n"
          ]
        }
      ],
      "source": [
        "for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[3][\"input_ids\"]), training_set[3][\"labels\"]):\n",
        "  print('{0:10}  {1}'.format(token, label))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntEzGdRMjVzL"
      },
      "outputs": [],
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "val_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': TEST_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "val_loader = DataLoader(val_set, **val_params)\n",
        "test_loader = DataLoader(test_set, **test_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkNhlqUI43pt",
        "outputId": "e58a5da7-e99c-495d-8d9a-6beb0e4078e2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=19, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4PsFZH8i837"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def focal_loss(logits, labels, alpha=0.25, gamma=2.0, ignore_index=-100):\n",
        "    \"\"\"\n",
        "    logits: [batch_size, seq_len, num_labels] - model predictions\n",
        "    labels: [batch_size, seq_len] - ground truth labels\n",
        "    \"\"\"\n",
        "    # Calculate Cross Entropy Loss without reduction\n",
        "    ce_loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), reduction='none', ignore_index=ignore_index)\n",
        "\n",
        "    # Get the predictions\n",
        "    pred_probs = F.softmax(logits.view(-1, logits.size(-1)), dim=-1)\n",
        "    pred_class = labels.view(-1)\n",
        "\n",
        "    # focusing parameter\n",
        "    gamma = gamma\n",
        "\n",
        "    # Filter out 'ignore_index' labels\n",
        "    filtered = labels.view(-1) != ignore_index\n",
        "\n",
        "    # Calculate focal loss\n",
        "    ce_loss_filtered = ce_loss[filtered]\n",
        "    pred_probs_filtered = pred_probs[filtered]\n",
        "    pred_class_filtered = pred_class[filtered]\n",
        "\n",
        "    # Construct the loss\n",
        "    pt = pred_probs_filtered.gather(1, pred_class_filtered.unsqueeze(-1)).squeeze()\n",
        "    loss = ((1 - pt) ** gamma * ce_loss_filtered).mean()  # mean over the batch\n",
        "\n",
        "    return loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaOtXFnGQgqz"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99XcALDnQk_K"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    tr_loss, tr_accuracy = 0, 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    tr_preds, tr_labels = [], []\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for idx, batch in enumerate(training_loader):\n",
        "        ids = batch['input_ids'].to(device, dtype=torch.long)\n",
        "        mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
        "        labels = batch['labels'].to(device, dtype=torch.long)\n",
        "    #     global_attention_mask = torch.zeros(\n",
        "    # input_ids.shape, dtype=torch.long, device=input_ids.device\n",
        "\n",
        "        # Perform a forward pass to get the logits\n",
        "        outputs = model(input_ids=ids, attention_mask=mask)\n",
        "\n",
        "        # Calculate the loss using focal_loss function directly\n",
        "        loss = focal_loss(outputs.logits, labels)  # Use the focal_loss function here directly\n",
        "\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples += labels.size(0)\n",
        "\n",
        "        if idx % 100 == 0:\n",
        "            loss_step = tr_loss / nb_tr_steps\n",
        "            print(f\"Training loss per 100 training steps: {loss_step}\")\n",
        "\n",
        "        # Decoding the logits to compute accuracy\n",
        "        active_logits = outputs.logits.view(-1, model.num_labels)\n",
        "        flattened_predictions = torch.argmax(active_logits, axis=1)\n",
        "\n",
        "        # Ignoring the predictions of the padding tokens\n",
        "        active_accuracy = labels.view(-1) != -100\n",
        "        active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n",
        "\n",
        "        labels = torch.masked_select(active_labels, active_accuracy)\n",
        "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "\n",
        "        tr_labels.extend(labels.cpu().numpy())\n",
        "        tr_preds.extend(predictions.cpu().numpy())\n",
        "\n",
        "        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
        "        tr_accuracy += tmp_tr_accuracy\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=MAX_GRAD_NORM)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = tr_loss / nb_tr_steps\n",
        "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
        "    print(f\"Training loss epoch: {epoch_loss}\")\n",
        "    print(f\"Training accuracy epoch: {tr_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "AUrRodaYT-ZV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9yYcqE5Qn62",
        "outputId": "ec955424-7489-46e2-9017-7217e2670308"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 2.568286657333374\n",
            "Training loss per 100 training steps: 0.574299370777784\n",
            "Training loss per 100 training steps: 0.3873309833599968\n",
            "Training loss per 100 training steps: 0.30238581737317766\n",
            "Training loss per 100 training steps: 0.253419600553727\n",
            "Training loss per 100 training steps: 0.21598296222533053\n",
            "Training loss per 100 training steps: 0.19152682534441856\n",
            "Training loss per 100 training steps: 0.17239579854301573\n",
            "Training loss per 100 training steps: 0.15764116292731306\n",
            "Training loss per 100 training steps: 0.1456252068481961\n",
            "Training loss per 100 training steps: 0.13575826032349111\n",
            "Training loss per 100 training steps: 0.12777430759723823\n",
            "Training loss per 100 training steps: 0.12102513165098013\n",
            "Training loss per 100 training steps: 0.11468873203976894\n",
            "Training loss per 100 training steps: 0.10942050983410044\n",
            "Training loss per 100 training steps: 0.10475875944507834\n",
            "Training loss per 100 training steps: 0.10049649704434443\n",
            "Training loss epoch: 0.09758595779014737\n",
            "Training accuracy epoch: 0.9646964784489277\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.06050063669681549\n",
            "Training loss per 100 training steps: 0.03056978749106684\n",
            "Training loss per 100 training steps: 0.02992854028504657\n",
            "Training loss per 100 training steps: 0.02962651281258667\n",
            "Training loss per 100 training steps: 0.029667188796073087\n",
            "Training loss per 100 training steps: 0.029665929022265558\n",
            "Training loss per 100 training steps: 0.02909823179952688\n",
            "Training loss per 100 training steps: 0.02881523329598036\n",
            "Training loss per 100 training steps: 0.029392944434452766\n",
            "Training loss per 100 training steps: 0.02928606811674581\n",
            "Training loss per 100 training steps: 0.0292846357759201\n",
            "Training loss per 100 training steps: 0.029178394321216636\n",
            "Training loss per 100 training steps: 0.028964686744833948\n",
            "Training loss per 100 training steps: 0.029111644200133766\n",
            "Training loss per 100 training steps: 0.028787593713548023\n",
            "Training loss per 100 training steps: 0.028672274900188286\n",
            "Training loss per 100 training steps: 0.02831426613560777\n",
            "Training loss epoch: 0.028419950520714568\n",
            "Training accuracy epoch: 0.9818487961235762\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.0038503562100231647\n",
            "Training loss per 100 training steps: 0.01935111577904986\n",
            "Training loss per 100 training steps: 0.017984149771231684\n",
            "Training loss per 100 training steps: 0.018248136334764797\n",
            "Training loss per 100 training steps: 0.019476710944050374\n",
            "Training loss per 100 training steps: 0.019522277032088014\n",
            "Training loss per 100 training steps: 0.01982456864306082\n",
            "Training loss per 100 training steps: 0.019953921116799594\n",
            "Training loss per 100 training steps: 0.020039558707868763\n",
            "Training loss per 100 training steps: 0.020235322971022224\n",
            "Training loss per 100 training steps: 0.020319268256851222\n",
            "Training loss per 100 training steps: 0.02055099590917258\n",
            "Training loss per 100 training steps: 0.020726363129549973\n",
            "Training loss per 100 training steps: 0.021025254887179604\n",
            "Training loss per 100 training steps: 0.02101319843374724\n",
            "Training loss per 100 training steps: 0.020787640482284105\n",
            "Training loss per 100 training steps: 0.020762582663102908\n",
            "Training loss epoch: 0.020908226076477784\n",
            "Training accuracy epoch: 0.9846421358369195\n",
            "Training epoch: 4\n",
            "Training loss per 100 training steps: 0.01297547947615385\n",
            "Training loss per 100 training steps: 0.016199231835668283\n",
            "Training loss per 100 training steps: 0.01608753202282372\n",
            "Training loss per 100 training steps: 0.016610395396563404\n",
            "Training loss per 100 training steps: 0.01697644374740195\n",
            "Training loss per 100 training steps: 0.017094268282306587\n",
            "Training loss per 100 training steps: 0.01682394624661775\n",
            "Training loss per 100 training steps: 0.016721864784567865\n",
            "Training loss per 100 training steps: 0.016749853224591812\n",
            "Training loss per 100 training steps: 0.016941611417364427\n",
            "Training loss per 100 training steps: 0.016828843486357735\n",
            "Training loss per 100 training steps: 0.0167477246685205\n",
            "Training loss per 100 training steps: 0.016848158594298907\n",
            "Training loss per 100 training steps: 0.016881002571177778\n",
            "Training loss per 100 training steps: 0.016736362228695775\n",
            "Training loss per 100 training steps: 0.01674263652000968\n",
            "Training loss per 100 training steps: 0.016708367103006536\n",
            "Training loss epoch: 0.016590604521968942\n",
            "Training accuracy epoch: 0.98634629470081\n",
            "Training epoch: 5\n",
            "Training loss per 100 training steps: 0.01599235087633133\n",
            "Training loss per 100 training steps: 0.01198507583813078\n",
            "Training loss per 100 training steps: 0.012579652058268753\n",
            "Training loss per 100 training steps: 0.012595323162045844\n",
            "Training loss per 100 training steps: 0.012446386260087915\n",
            "Training loss per 100 training steps: 0.012835398049859661\n",
            "Training loss per 100 training steps: 0.01304724473963738\n",
            "Training loss per 100 training steps: 0.013101023468140885\n",
            "Training loss per 100 training steps: 0.013156418638456135\n",
            "Training loss per 100 training steps: 0.013291301442656956\n",
            "Training loss per 100 training steps: 0.013180853159836943\n",
            "Training loss per 100 training steps: 0.01302123039705251\n",
            "Training loss per 100 training steps: 0.013101656537566227\n",
            "Training loss per 100 training steps: 0.013161707515998159\n",
            "Training loss per 100 training steps: 0.013176439329671498\n",
            "Training loss per 100 training steps: 0.013200134346891832\n",
            "Training loss per 100 training steps: 0.01314716810767282\n",
            "Training loss epoch: 0.01314377051620697\n",
            "Training accuracy epoch: 0.9885062672598536\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Training epoch: {epoch + 1}\")\n",
        "    train(epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validate"
      ],
      "metadata": {
        "id": "7Xi8xRIpT_9n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiUuvNdBldmT"
      },
      "outputs": [],
      "source": [
        "def valid(model, testing_loader):\n",
        "    model.eval()\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "\n",
        "    nb_eval_examples, nb_eval_steps = 0, 0\n",
        "    eval_preds, eval_labels, indices = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(testing_loader):\n",
        "            ids = batch['input_ids'].to(device, dtype=torch.long)\n",
        "            mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
        "            labels = batch['labels'].to(device, dtype=torch.long)\n",
        "            # indices_list = batch[idx]\n",
        "\n",
        "            outputs = model(input_ids=ids, attention_mask=mask)\n",
        "            loss = focal_loss(outputs.logits, labels)  # Use the same focal_loss function\n",
        "\n",
        "            eval_loss += loss.item()\n",
        "            nb_eval_steps += 1\n",
        "            nb_eval_examples += labels.size(0)\n",
        "\n",
        "            if idx % 100 == 0:\n",
        "                loss_step = eval_loss / nb_eval_steps\n",
        "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
        "\n",
        "            # Decoding the logits to compute accuracy\n",
        "            active_logits = outputs.logits.view(-1, model.num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1)\n",
        "\n",
        "            # Ignoring the predictions of the padding tokens\n",
        "            active_accuracy = labels.view(-1) != -100\n",
        "            active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n",
        "\n",
        "            labels = torch.masked_select(active_labels, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "\n",
        "            eval_labels.extend(labels.cpu().numpy())\n",
        "            eval_preds.extend(predictions.cpu().numpy())\n",
        "\n",
        "            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "            # indices.extend(indices_list)\n",
        "\n",
        "\n",
        "\n",
        "    labels = [ids_to_labels[id.item()] for id in eval_labels]\n",
        "    predictions = [ids_to_labels[id.item()] for id in eval_preds]\n",
        "\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
        "    print(f\"Validation Loss: {eval_loss}\")\n",
        "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
        "\n",
        "    return labels, predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gs3qABW2_Z-V",
        "outputId": "84d256ee-20c8-4dd1-e119-542bfabdc5ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss per 100 evaluation steps: 0.00022166207781992853\n",
            "Validation loss per 100 evaluation steps: 0.03498244163279871\n",
            "Validation loss per 100 evaluation steps: 0.03138240548946556\n",
            "Validation loss per 100 evaluation steps: 0.032715125576556224\n",
            "Validation loss per 100 evaluation steps: 0.033668690335490255\n",
            "Validation loss per 100 evaluation steps: 0.032859125436865484\n",
            "Validation loss per 100 evaluation steps: 0.032535931935109855\n",
            "Validation loss per 100 evaluation steps: 0.03285741111598179\n",
            "Validation loss per 100 evaluation steps: 0.032524934582029236\n",
            "Validation Loss: 0.032050740191022255\n",
            "Validation Accuracy: 0.9784199941138852\n"
          ]
        }
      ],
      "source": [
        "labels, predictions = valid(model, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIcbSKb79_2B",
        "outputId": "1695e848-5818-4823-934f-81ca2a184e64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADE       0.36      0.55      0.43       165\n",
            "      Dosage       0.84      0.90      0.87       782\n",
            "        Drug       0.88      0.93      0.91      3216\n",
            "    Duration       0.64      0.80      0.71       120\n",
            "        Form       0.90      0.88      0.89      1269\n",
            "   Frequency       0.78      0.82      0.80      1270\n",
            "      Reason       0.54      0.70      0.61       698\n",
            "       Route       0.91      0.92      0.92      1143\n",
            "    Strength       0.93      0.93      0.93      1341\n",
            "\n",
            "   micro avg       0.83      0.88      0.86     10004\n",
            "   macro avg       0.75      0.83      0.79     10004\n",
            "weighted avg       0.84      0.88      0.86     10004\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from seqeval.metrics import classification_report\n",
        "\n",
        "print(classification_report([labels], [predictions]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Set"
      ],
      "metadata": {
        "id": "-r3I62WQTlJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels, test_predictions = valid(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INLdCF2yhcUg",
        "outputId": "d3459dd0-c66b-41e5-96ad-71e56aee2c76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss per 100 evaluation steps: 0.05939135327935219\n",
            "Validation loss per 100 evaluation steps: 0.026654486253968265\n",
            "Validation loss per 100 evaluation steps: 0.025752847699161193\n",
            "Validation loss per 100 evaluation steps: 0.02652863734417233\n",
            "Validation Loss: 0.027112228247499825\n",
            "Validation Accuracy: 0.9809151480306614\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import classification_report\n",
        "\n",
        "print(classification_report([test_labels], [test_predictions]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kgpCwberb1-",
        "outputId": "0eb614dd-2dc4-4fc0-d16b-1694a651eead"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADE       0.43      0.53      0.48       634\n",
            "      Dosage       0.89      0.93      0.91      2704\n",
            "        Drug       0.91      0.94      0.92     10595\n",
            "    Duration       0.62      0.74      0.68       385\n",
            "        Form       0.93      0.92      0.92      4373\n",
            "   Frequency       0.83      0.85      0.84      4151\n",
            "      Reason       0.54      0.66      0.59      2561\n",
            "       Route       0.93      0.93      0.93      3513\n",
            "    Strength       0.95      0.96      0.95      4237\n",
            "\n",
            "   micro avg       0.86      0.89      0.88     33153\n",
            "   macro avg       0.78      0.83      0.80     33153\n",
            "weighted avg       0.87      0.89      0.88     33153\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccgbCa8siNo7"
      },
      "source": [
        "### Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4564VeJwyoj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "directory = \"/content/drive/MyDrive/266_final/microsoft_bbb_concat7\"\n",
        "\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "tokenizer.save_vocabulary(directory)\n",
        "model.save_pretrained(directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDOInmcW3qwe"
      },
      "source": [
        "Resources\n",
        "\n",
        "### https://github.com/lcampillos/Medical-NER/blob/master/bert_ner.ipynb\n",
        "### https://medium.com/analytics-vidhya/bio-tagged-text-to-original-text-99b05da6664"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
