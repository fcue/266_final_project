{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DixhWmqWcy3M"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Mehq_mVQ59mk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYBZ86dRc2bq",
        "outputId": "1e9b12c7-e017-4cb2-ffa0-fd880466626d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BYwLmovB5q4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verify GPU\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPL27mL0_sTC",
        "outputId": "7a8d7b50-60e8-4db9-a486-9e5145a217db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert to DataFrame for EDA"
      ],
      "metadata": {
        "id": "qyMy7PgZ_mME"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJ6HnnrvKF_q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the data from the text file\n",
        "with open(\"/content/drive/MyDrive/266_final/data/dataset1_train.txt\", \"r\") as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Define an empty list to store the data\n",
        "data = []\n",
        "\n",
        "# Iterate over each line in the file\n",
        "for line in lines:\n",
        "    # Split the line by spaces\n",
        "    parts = line.strip().split()\n",
        "\n",
        "    # Check if the line has the expected number of elements\n",
        "    if len(parts) == 9:\n",
        "        # Extract the values from the line\n",
        "        text_file_name = parts[0]\n",
        "        sentence_line_number = int(parts[1])\n",
        "        sentence_word_index = int(parts[2])\n",
        "        sentence_seq = parts[3]\n",
        "        start_token = int(parts[4])\n",
        "        end_token = int(parts[5])\n",
        "        original_word = parts[6]\n",
        "        word = parts[7]\n",
        "        label = parts[8]\n",
        "\n",
        "        # Append the values as a tuple to the data list\n",
        "        data.append((text_file_name, sentence_line_number, sentence_word_index, sentence_seq,\n",
        "                     start_token, end_token, original_word, word, label))\n",
        "\n",
        "# Create a DataFrame from the data list with appropriate column names\n",
        "df = pd.DataFrame(data, columns=['text_file_name', 'sentence_line_number', 'sentence_word_index',\n",
        "                                 'sentence_seq', 'start_token', 'end_token', 'original_word',\n",
        "                                 'word', 'label'])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBwbw0ddKYcJ"
      },
      "outputs": [],
      "source": [
        "def convert_data(filepath):\n",
        "\n",
        "  # Read the data from the text file\n",
        "  with open(filepath, \"r\") as file:\n",
        "      lines = file.readlines()\n",
        "\n",
        "  # Define an empty list to store the data\n",
        "  data = []\n",
        "\n",
        "  # Iterate over each line in the file\n",
        "  for line in lines:\n",
        "      # Split the line by spaces\n",
        "      parts = line.strip().split()\n",
        "\n",
        "      # Check if the line has the expected number of elements\n",
        "      if len(parts) == 9:\n",
        "          # Extract the values from the line\n",
        "          text_file_name = parts[0]\n",
        "          sentence_line_number = int(parts[1])\n",
        "          sentence_word_index = int(parts[2])\n",
        "          sentence_seq = parts[3]\n",
        "          start_token = int(parts[4])\n",
        "          end_token = int(parts[5])\n",
        "          original_word = parts[6]\n",
        "          word = parts[7]\n",
        "          label = parts[8]\n",
        "\n",
        "          # Append the values as a tuple to the data list\n",
        "          data.append((text_file_name, sentence_line_number, sentence_word_index, sentence_seq,\n",
        "                      start_token, end_token, original_word, word, label))\n",
        "\n",
        "  # Create a DataFrame from the data list with appropriate column names\n",
        "  df = pd.DataFrame(data, columns=['text_file_name', 'sentence_line_number', 'sentence_word_index',\n",
        "                                  'sentence_seq', 'start_token', 'end_token', 'original_word',\n",
        "                                  'word', 'label'])\n",
        "\n",
        "  return df\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_path = \"/content/drive/MyDrive/266_final/data/dataset1_train.txt\"\n",
        "test_data_path = \"/content/drive/MyDrive/266_final/data/dataset1_test.txt\"\n",
        "\n",
        "train = convert_data(train_data_path)\n",
        "test = convert_data(test_data_path)\n",
        "\n",
        "print(f\"Length of train: {len(train)}\")\n",
        "print(f\"Length of test: {len(test)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97zEc_c_qYWJ",
        "outputId": "92d2ab93-4a1c-4d77-82f0-0b694de650fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of train: 895141\n",
            "Length of test: 585761\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = train.copy()"
      ],
      "metadata": {
        "id": "ipR22Y9PrElo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['sentence_line_number'].nunique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqnvelmiGLXi",
        "outputId": "a36d2cb7-8bee-4b93-96b4-2b6acbfba569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1053"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVvq1qgWw1vR",
        "outputId": "2a3ea6a7-2cf8-44e2-9f81-f5d4ac8da5a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "O              802045\n",
              "B-Drug          16222\n",
              "I-Frequency     13023\n",
              "I-Dosage         8779\n",
              "B-Strength       6691\n",
              "B-Form           6647\n",
              "I-Strength       6617\n",
              "B-Frequency      6279\n",
              "B-Route          5475\n",
              "I-Drug           4298\n",
              "B-Dosage         4221\n",
              "I-Form           4173\n",
              "B-Reason         3791\n",
              "I-Reason         3125\n",
              "I-Duration       1034\n",
              "B-ADE             956\n",
              "I-ADE             776\n",
              "B-Duration        592\n",
              "I-Route           397\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['sentence_line_number'] == 1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "s1k_AqE-MHR0",
        "outputId": "ddcfa4aa-8fa5-46b6-b380-39fdf267541d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                           text_file_name  sentence_line_number  \\\n",
              "0       data/training_20180910/110727.txt                     1   \n",
              "1       data/training_20180910/110727.txt                     1   \n",
              "2       data/training_20180910/110727.txt                     1   \n",
              "3       data/training_20180910/110727.txt                     1   \n",
              "4       data/training_20180910/110727.txt                     1   \n",
              "...                                   ...                   ...   \n",
              "893314  data/training_20180910/100883.txt                     1   \n",
              "893315  data/training_20180910/100883.txt                     1   \n",
              "893316  data/training_20180910/100883.txt                     1   \n",
              "893317  data/training_20180910/100883.txt                     1   \n",
              "893318  data/training_20180910/100883.txt                     1   \n",
              "\n",
              "        sentence_word_index sentence_seq  start_token  end_token  \\\n",
              "0                         0           NA            0          9   \n",
              "1                         1           NA           10         14   \n",
              "2                         2           NA           14         15   \n",
              "3                         3           NA           17         18   \n",
              "4                         4           NA           18         19   \n",
              "...                     ...          ...          ...        ...   \n",
              "893314                   23           NA           64         65   \n",
              "893315                   24           NA           65         66   \n",
              "893316                   25           NA           66         67   \n",
              "893317                   26           NA           67         68   \n",
              "893318                   27           NA           68         69   \n",
              "\n",
              "       original_word       word label  \n",
              "0          Admission  Admission     O  \n",
              "1               Date       Date     O  \n",
              "2                  :          :     O  \n",
              "3                  [          [     O  \n",
              "4                  *          *     O  \n",
              "...              ...        ...   ...  \n",
              "893314             -          -     O  \n",
              "893315             3    ORDINAL     O  \n",
              "893316             *          *     O  \n",
              "893317             *          *     O  \n",
              "893318             ]          ]     O  \n",
              "\n",
              "[8547 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-32bd3496-f1ef-4f53-a2f5-7462cbea384e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_file_name</th>\n",
              "      <th>sentence_line_number</th>\n",
              "      <th>sentence_word_index</th>\n",
              "      <th>sentence_seq</th>\n",
              "      <th>start_token</th>\n",
              "      <th>end_token</th>\n",
              "      <th>original_word</th>\n",
              "      <th>word</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>data/training_20180910/110727.txt</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NA</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>Admission</td>\n",
              "      <td>Admission</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>data/training_20180910/110727.txt</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>NA</td>\n",
              "      <td>10</td>\n",
              "      <td>14</td>\n",
              "      <td>Date</td>\n",
              "      <td>Date</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>data/training_20180910/110727.txt</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>NA</td>\n",
              "      <td>14</td>\n",
              "      <td>15</td>\n",
              "      <td>:</td>\n",
              "      <td>:</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>data/training_20180910/110727.txt</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>NA</td>\n",
              "      <td>17</td>\n",
              "      <td>18</td>\n",
              "      <td>[</td>\n",
              "      <td>[</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>data/training_20180910/110727.txt</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>NA</td>\n",
              "      <td>18</td>\n",
              "      <td>19</td>\n",
              "      <td>*</td>\n",
              "      <td>*</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>893314</th>\n",
              "      <td>data/training_20180910/100883.txt</td>\n",
              "      <td>1</td>\n",
              "      <td>23</td>\n",
              "      <td>NA</td>\n",
              "      <td>64</td>\n",
              "      <td>65</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>893315</th>\n",
              "      <td>data/training_20180910/100883.txt</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>NA</td>\n",
              "      <td>65</td>\n",
              "      <td>66</td>\n",
              "      <td>3</td>\n",
              "      <td>ORDINAL</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>893316</th>\n",
              "      <td>data/training_20180910/100883.txt</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>NA</td>\n",
              "      <td>66</td>\n",
              "      <td>67</td>\n",
              "      <td>*</td>\n",
              "      <td>*</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>893317</th>\n",
              "      <td>data/training_20180910/100883.txt</td>\n",
              "      <td>1</td>\n",
              "      <td>26</td>\n",
              "      <td>NA</td>\n",
              "      <td>67</td>\n",
              "      <td>68</td>\n",
              "      <td>*</td>\n",
              "      <td>*</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>893318</th>\n",
              "      <td>data/training_20180910/100883.txt</td>\n",
              "      <td>1</td>\n",
              "      <td>27</td>\n",
              "      <td>NA</td>\n",
              "      <td>68</td>\n",
              "      <td>69</td>\n",
              "      <td>]</td>\n",
              "      <td>]</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8547 rows Ã— 9 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-32bd3496-f1ef-4f53-a2f5-7462cbea384e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-32bd3496-f1ef-4f53-a2f5-7462cbea384e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-32bd3496-f1ef-4f53-a2f5-7462cbea384e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ddf19b89-716d-4c1d-887a-5b7069375705\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ddf19b89-716d-4c1d-887a-5b7069375705')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ddf19b89-716d-4c1d-887a-5b7069375705 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df[df['sentence_line_number'] == 1]\",\n  \"rows\": 8547,\n  \"fields\": [\n    {\n      \"column\": \"text_file_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 303,\n        \"samples\": [\n          \"data/training_20180910/125867.txt\",\n          \"data/training_20180910/102173.txt\",\n          \"data/training_20180910/186876.txt\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence_line_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence_word_index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 0,\n        \"max\": 32,\n        \"num_unique_values\": 33,\n        \"samples\": [\n          31\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence_seq\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"NA\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"start_token\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 25,\n        \"min\": 0,\n        \"max\": 119,\n        \"num_unique_values\": 106,\n        \"samples\": [\n          40\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"end_token\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 25,\n        \"min\": 4,\n        \"max\": 120,\n        \"num_unique_values\": 105,\n        \"samples\": [\n          61\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"original_word\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 177,\n        \"samples\": [\n          \"12\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"word\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 19,\n        \"samples\": [\n          \"Admission\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"O\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Label labels"
      ],
      "metadata": {
        "id": "GWzDzNtXJRSa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXnnrZqSIX95",
        "outputId": "6ffa578f-2986-4e1a-f3c4-9c27a2c7f2f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'I-ADE', 'B-Strength', 'I-Form', 'B-Drug', 'I-Route', 'I-Dosage', 'B-ADE', 'B-Frequency', 'B-Route', 'B-Reason', 'I-Duration', 'B-Duration', 'B-Form', 'O', 'I-Reason', 'I-Drug', 'I-Frequency', 'I-Strength', 'B-Dosage'}\n"
          ]
        }
      ],
      "source": [
        "# Split labels based on whitespace and turn them into a list\n",
        "labels = [i.split() for i in df['label'].values.tolist()]\n",
        "\n",
        "# Check how many labels are there in the dataset\n",
        "unique_labels = set()\n",
        "\n",
        "for lb in labels:\n",
        "  [unique_labels.add(i) for i in lb if i not in unique_labels]\n",
        "\n",
        "print(unique_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ux83z2FGLU05",
        "outputId": "d5d38bf9-9d16-4e03-e8eb-9fe4847f9cd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'B-ADE': 0, 'B-Dosage': 1, 'B-Drug': 2, 'B-Duration': 3, 'B-Form': 4, 'B-Frequency': 5, 'B-Reason': 6, 'B-Route': 7, 'B-Strength': 8, 'I-ADE': 9, 'I-Dosage': 10, 'I-Drug': 11, 'I-Duration': 12, 'I-Form': 13, 'I-Frequency': 14, 'I-Reason': 15, 'I-Route': 16, 'I-Strength': 17, 'O': 18}\n"
          ]
        }
      ],
      "source": [
        "# Map each label into its id representation and vice versa\n",
        "labels_to_ids = {k: v for v, k in enumerate(sorted(unique_labels))}\n",
        "ids_to_labels = {v: k for v, k in enumerate(sorted(unique_labels))}\n",
        "print(labels_to_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experimenting with formats for BERT"
      ],
      "metadata": {
        "id": "E9FKpl2hJMXZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_WOoORyS6VX",
        "outputId": "5e330fd8-0282-48f0-bca1-c476ae975fa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Admission', 'Date', ':', '[', '*', '*', '2202', '-', '1', '-', '8', '*', '*', ']', 'Discharge', 'Date', ':', '[', '*', '*', '2202', '-', '2', '-', '1', '*', '*', ']'], ['Date', 'of', 'Birth', ':', '[', '*', '*', '2163', '-', '9', '-', '18', '*', '*', ']', 'Sex', ':', 'M'], ['Service', ':', 'MEDICINE'], ['Allergies', ':', 'Keflex', '/', 'Orencia', '/', 'Remicade'], ['Attending', ':', '[', '*', '*', 'First', 'Name3', '(', 'LF', ')', '2751', '*', '*', ']', 'Chief', 'Complaint', ':', 'L', 'leg', 'pain', 'and', 'erythema']]\n",
            "[['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O'], ['O', 'O', 'B-Drug', 'O', 'B-Drug', 'O', 'B-Drug'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"
          ]
        }
      ],
      "source": [
        "train_texts = []\n",
        "train_labels = []\n",
        "\n",
        "# Read the text file line by line\n",
        "with open('/content/drive/MyDrive/266_final/data/dataset1_train.txt', 'r', encoding='utf-8') as file:\n",
        "    current_text = []  # To store tokens of the current text\n",
        "    current_labels = []  # To store labels of the current text\n",
        "    for line in file:\n",
        "        if line.strip() == '':  # Empty line signifies end of text\n",
        "            train_texts.append(current_text)\n",
        "            train_labels.append(current_labels)\n",
        "            current_text = []\n",
        "            current_labels = []\n",
        "        else:\n",
        "            parts = line.strip().split()\n",
        "            token = parts[-3]  # Token is second-to-last part\n",
        "            label = parts[-1]   # Label is last part\n",
        "            current_text.append(token)\n",
        "            current_labels.append(label)\n",
        "\n",
        "# Check the first few samples\n",
        "print(train_texts[:5])\n",
        "print(train_labels[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgC8uV1RyBFk",
        "outputId": "e7c335e8-ced1-4a8c-d847-1d88dfecb6fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Attending',\n",
              " ':',\n",
              " '[',\n",
              " '*',\n",
              " '*',\n",
              " 'First',\n",
              " 'Name3',\n",
              " '(',\n",
              " 'LF',\n",
              " ')',\n",
              " '2751',\n",
              " '*',\n",
              " '*',\n",
              " ']',\n",
              " 'Chief',\n",
              " 'Complaint',\n",
              " ':',\n",
              " 'L',\n",
              " 'leg',\n",
              " 'pain',\n",
              " 'and',\n",
              " 'erythema']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "train_texts[4]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels[4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpEIYNpZIIfo",
        "outputId": "ced4483d-067f-440b-e058-c032742fea57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Not sure if this is the correct format. Let's look into another way to turn it into a sentence."
      ],
      "metadata": {
        "id": "88Q4ui6OJgGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def formatted_df(df):\n",
        "  df['sentence'] = df[[\n",
        "      'text_file_name',\n",
        "      'sentence_line_number',\n",
        "      'original_word',\n",
        "      'label']].groupby(\n",
        "          ['text_file_name', 'sentence_line_number'])['original_word'].transform(lambda x: ' '.join(x))\n",
        "\n",
        "  df['word_labels'] = df[[\n",
        "      'text_file_name',\n",
        "      'sentence_line_number',\n",
        "      'original_word',\n",
        "      'label']].groupby(\n",
        "          ['text_file_name', 'sentence_line_number'])['label'].transform(lambda x: ','.join(x))\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "bprwS-tWrRdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = formatted_df(df)\n",
        "test = formatted_df(test)"
      ],
      "metadata": {
        "id": "qNJIWjL1sGg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df['sentence'] = df[['text_file_name', 'sentence_line_number', 'original_word', 'label']].groupby(['text_file_name', 'sentence_line_number'])['original_word'].transform(lambda x: ' '.join(x))"
      ],
      "metadata": {
        "id": "aQtuu6Cy448o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df['word_labels'] = df[['text_file_name', 'sentence_line_number', 'original_word', 'label']].groupby(['text_file_name', 'sentence_line_number'])['label'].transform(lambda x: ','.join(x))\n"
      ],
      "metadata": {
        "id": "Y5-hIDJW8_XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_level_data(df):\n",
        "  sentence_level_data = df[[\"sentence\", \"word_labels\"]].drop_duplicates().reset_index(drop=True)\n",
        "  return sentence_level_data"
      ],
      "metadata": {
        "id": "l3zrCBf-sfe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sentence_level_data = df[[\"sentence\", \"word_labels\"]].drop_duplicates().reset_index(drop=True)\n",
        "# sentence_level_data.head()"
      ],
      "metadata": {
        "id": "XpGwter59c47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_level_train = sentence_level_data(df)\n",
        "sentence_level_test = sentence_level_data(test)"
      ],
      "metadata": {
        "id": "O2f8oq0fssJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_level_train.iloc[50].sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NfcU3rIg-x94",
        "outputId": "0f8b423e-230c-435a-c4f7-85504b752b95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'otherwise negative .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_level_train.iloc[50].word_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hyBQMf9x-lcM",
        "outputId": "9924196c-c0ef-410b-d5fa-fa1864f4e253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'O,O,O'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_level_test.iloc[50].sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TCWptRAmtCQ4",
        "outputId": "6e9b7e14-fd35-4c93-9c4f-49c1d653e4c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'NECK : unable to asses JVP 2/2 habitus'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_level_test.iloc[50].word_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6kZ0XxkxtFPe",
        "outputId": "e779904d-1a0d-4a1a-be04-481f3ab4d656"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'O,O,O,O,O,O,O,O'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = sentence_level_train.copy()\n",
        "data_test = sentence_level_test.copy()"
      ],
      "metadata": {
        "id": "745qa3MhQpM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### It looks reasonable, let's proceed with training"
      ],
      "metadata": {
        "id": "oIRanj57PpZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast, BertConfig, BertForTokenClassification\n",
        "import os\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8pmcdPWk_k5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import BertTokenizerFast, BertConfig, BertForTokenClassification\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "model_name = \"emilyalsentzer/Bio_ClinicalBERT\"  # Bio_ClinicalBERT model identifier\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(labels_to_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjoXu_EoD1O_",
        "outputId": "31f47649-a994-4d22-b6a8-79a71e582621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 4\n",
        "VALID_BATCH_SIZE = 2\n",
        "TEST_BATCH_SIZE = 32\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 1e-05\n",
        "MAX_GRAD_NORM = 10\n",
        "# tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n"
      ],
      "metadata": {
        "id": "VRjrBQAR9-zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class dataset(Dataset):\n",
        "  def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        # step 1: get the sentence and word labels\n",
        "        sentence = self.data.sentence[index].strip().split()\n",
        "        word_labels = self.data.word_labels[index].split(\",\")\n",
        "\n",
        "        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n",
        "        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n",
        "        encoding = self.tokenizer(sentence,\n",
        "                             return_offsets_mapping=True,\n",
        "                             padding='max_length',\n",
        "                             is_split_into_words=True,\n",
        "                             truncation=True,\n",
        "                             max_length=self.max_len)\n",
        "\n",
        "        # step 3: create token labels only for first word pieces of each tokenized word\n",
        "        labels = [labels_to_ids[label] for label in word_labels]\n",
        "        # code based on https://huggingface.co/transformers/custom_datasets.html#tok-ner\n",
        "        # create an empty array of -100 of length max_length\n",
        "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
        "\n",
        "        # set only labels whose first offset position is 0 and the second is not 0\n",
        "        i = 0\n",
        "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
        "          if mapping[0] == 0 and mapping[1] != 0:\n",
        "            # overwrite label\n",
        "            encoded_labels[idx] = labels[i]\n",
        "            i += 1\n",
        "\n",
        "        # step 4: turn everything into PyTorch tensors\n",
        "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
        "        item['labels'] = torch.as_tensor(encoded_labels)\n",
        "\n",
        "        return item\n",
        "\n",
        "  def __len__(self):\n",
        "        return self.len"
      ],
      "metadata": {
        "id": "rYVPN8o4ve9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 0.8\n",
        "train_dataset = data_train.sample(frac=train_size,random_state=200)\n",
        "val_dataset = data_train.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(data_train.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"Val Dataset: {}\".format(val_dataset.shape))\n",
        "print(\"Test Dataset: {}\".format(data_test.shape))\n",
        "\n",
        "\n",
        "training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
        "val_set = dataset(val_dataset, tokenizer, MAX_LEN)\n",
        "test_set = dataset(data_test, tokenizer, MAX_LEN)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDFvMkzgQjUX",
        "outputId": "b2420ef3-e680-4d70-9101-6003f3badcb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FULL Dataset: (66944, 2)\n",
            "TRAIN Dataset: (53555, 2)\n",
            "Val Dataset: (13389, 2)\n",
            "Test Dataset: (45034, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_set[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49Zb6f0F0Ru3",
        "outputId": "781a31cc-bdde-4e90-9cee-e021c0d77b0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([  101,  3879,   119,  1224,  1142,  1285,  1119,  1108,  1195,  6354,\n",
              "          1181,  1121, 14516, 13759,   117, 21167,   102,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
              " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'offset_mapping': tensor([[0, 0],\n",
              "         [0, 9],\n",
              "         [0, 1],\n",
              "         [0, 5],\n",
              "         [0, 4],\n",
              "         [0, 3],\n",
              "         [0, 2],\n",
              "         [0, 3],\n",
              "         [0, 2],\n",
              "         [2, 5],\n",
              "         [5, 6],\n",
              "         [0, 4],\n",
              "         [0, 2],\n",
              "         [2, 8],\n",
              "         [0, 1],\n",
              "         [0, 5],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0]]),\n",
              " 'labels': tensor([-100,   18,   18,   18,   18,   18,   18,   18,   18, -100, -100,   18,\n",
              "           18, -100,   18,   18, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100])}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_set[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reyjdAMAyFAl",
        "outputId": "8ca55f93-f8d4-4b91-a31f-31c95bb5998d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([  101, 10296,  2236,   131,   164,   115,   115, 19538,  1477,   118,\n",
              "           122,   118,  1489,   115,   115,   166, 12398,  2236,   131,   164,\n",
              "           115,   115, 19538,  1477,   118,   123,   118,   127,   115,   115,\n",
              "           166,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
              " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'offset_mapping': tensor([[0, 0],\n",
              "         [0, 9],\n",
              "         [0, 4],\n",
              "         [0, 1],\n",
              "         [0, 1],\n",
              "         [0, 1],\n",
              "         [0, 1],\n",
              "         [0, 3],\n",
              "         [3, 4],\n",
              "         [0, 1],\n",
              "         [0, 1],\n",
              "         [0, 1],\n",
              "         [0, 2],\n",
              "         [0, 1],\n",
              "         [0, 1],\n",
              "         [0, 1],\n",
              "         [0, 9],\n",
              "         [0, 4],\n",
              "         [0, 1],\n",
              "         [0, 1],\n",
              "         [0, 1],\n",
              "         [0, 1],\n",
              "         [0, 3],\n",
              "         [3, 4],\n",
              "         [0, 1],\n",
              "         [0, 1],\n",
              "         [0, 1],\n",
              "         [0, 1],\n",
              "         [0, 1],\n",
              "         [0, 1],\n",
              "         [0, 1],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0]]),\n",
              " 'labels': tensor([-100,   18,   18,   18,   18,   18,   18,   18, -100,   18,   18,   18,\n",
              "           18,   18,   18,   18,   18,   18,   18,   18,   18,   18,   18, -100,\n",
              "           18,   18,   18,   18,   18,   18,   18, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100])}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[0][\"input_ids\"]), training_set[0][\"labels\"]):\n",
        "  print('{0:10}  {1}'.format(token, label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4v9CdAR3b9b",
        "outputId": "60ab2199-3d0e-4ca8-f0a3-f84b83b5b157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS]       -100\n",
            "condition   18\n",
            ".           18\n",
            "later       18\n",
            "this        18\n",
            "day         18\n",
            "he          18\n",
            "was         18\n",
            "we          18\n",
            "##ane       -100\n",
            "##d         -100\n",
            "from        18\n",
            "se          18\n",
            "##dation    -100\n",
            ",           18\n",
            "awoke       18\n",
            "[SEP]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token, label in zip(tokenizer.convert_ids_to_tokens(test_set[0][\"input_ids\"]), test_set[0][\"labels\"]):\n",
        "  print('{0:10}  {1}'.format(token, label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjS39K3yyO0i",
        "outputId": "6830c498-ad1a-4401-cade-a6642d9c1e6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS]       -100\n",
            "admission   18\n",
            "date        18\n",
            ":           18\n",
            "[           18\n",
            "*           18\n",
            "*           18\n",
            "212         18\n",
            "##2         -100\n",
            "-           18\n",
            "1           18\n",
            "-           18\n",
            "14          18\n",
            "*           18\n",
            "*           18\n",
            "]           18\n",
            "discharge   18\n",
            "date        18\n",
            ":           18\n",
            "[           18\n",
            "*           18\n",
            "*           18\n",
            "212         18\n",
            "##2         -100\n",
            "-           18\n",
            "2           18\n",
            "-           18\n",
            "6           18\n",
            "*           18\n",
            "*           18\n",
            "]           18\n",
            "[SEP]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "val_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': TEST_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "val_loader = DataLoader(val_set, **val_params)\n",
        "testing_loader = DataLoader(test_set, **test_params)\n",
        "\n"
      ],
      "metadata": {
        "id": "zDNfrhJP4llk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pS2tYJ305jcG",
        "outputId": "2d89dfc8-b71e-41a8-ff3f-4a1f301ad01f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7d5dcaafd630>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(labels_to_ids))\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkNhlqUI43pt",
        "outputId": "89fc00d9-710e-4fc5-a657-460aedca6b8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=19, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# label_counts = train_dataset['word_label'].value_counts()\n",
        "# total_counts = sum(label_counts)\n",
        "# class_weights = total_counts / label_counts\n",
        "# class_weights_normalized = class_weights / class_weights.sum()\n",
        "# weights = torch.tensor(class_weights_normalized.values, dtype=torch.float).to(device)"
      ],
      "metadata": {
        "id": "lLaXGAU8z5bA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset['word_labels'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4pABE9y0rbi",
        "outputId": "bb3c30ef-69cc-4734-86bf-99d379cc224e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "O,O,O,O,O,O,O,O,O,O,O                                                                                                                        4290\n",
              "O,O,O,O,O,O,O,O,O,O                                                                                                                          3863\n",
              "O,O,O,O,O,O,O,O,O,O,O,O                                                                                                                      3565\n",
              "O,O,O,O,O,O,O,O,O                                                                                                                            2934\n",
              "O,O,O,O,O,O,O,O,O,O,O,O,O                                                                                                                    2630\n",
              "                                                                                                                                             ... \n",
              "B-Drug,O,B-Drug,I-Drug,I-Drug,I-Drug,I-Drug                                                                                                     1\n",
              "O,O,B-Drug,I-Drug,B-Strength,I-Strength,B-Form,I-Form,I-Form,I-Form,O,O,B-Dosage                                                                1\n",
              "O,B-Drug,B-Strength,I-Strength,I-Strength,I-Strength,I-Strength,I-Strength,O,O,O,O,O,O,O,O,O,O,O,B-Route,B-Frequency,I-Frequency,B-Reason       1\n",
              "O,O,O,O,O,O,O,O,O,O,O,B-Reason,I-Reason,I-Reason,O,B-Drug                                                                                       1\n",
              "B-Drug,I-Drug,I-Drug,I-Drug,O,B-Strength,I-Strength,B-Form,I-Form,I-Form,O                                                                      1\n",
              "Name: word_labels, Length: 7197, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DDMOZi8h0a5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = training_set[2]\n",
        "input_ids = inputs[\"input_ids\"].unsqueeze(0)\n",
        "attention_mask = inputs[\"attention_mask\"].unsqueeze(0)\n",
        "labels = inputs[\"labels\"].unsqueeze(0)\n",
        "\n",
        "input_ids = input_ids.to(device)\n",
        "attention_mask = attention_mask.to(device)\n",
        "labels = labels.to(device)\n",
        "\n",
        "outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "initial_loss = outputs[0]\n",
        "initial_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUh5g2Az5J7u",
        "outputId": "177c4c78-e941-4d96-c176-0bcf18ff8734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.9458, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tr_logits = outputs[1]\n",
        "tr_logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-qfx9lU6JqM",
        "outputId": "4e43d0ed-412b-4592-8c93-8afe9570e7d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 128, 19])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "CaOtXFnGQgqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def focal_loss(logits, labels, alpha=0.25, gamma=2.0, ignore_index=-100):\n",
        "    \"\"\"\n",
        "    logits: [batch_size, seq_len, num_labels] - model predictions\n",
        "    labels: [batch_size, seq_len] - ground truth labels\n",
        "    \"\"\"\n",
        "    # Calculate Cross Entropy Loss without reduction\n",
        "    ce_loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), reduction='none', ignore_index=ignore_index)\n",
        "\n",
        "    # Get the predictions\n",
        "    pred_probs = F.softmax(logits.view(-1, logits.size(-1)), dim=-1)\n",
        "    pred_class = labels.view(-1)\n",
        "\n",
        "    # focusing parameter\n",
        "    gamma = gamma\n",
        "\n",
        "    # Filter out 'ignore_index' labels\n",
        "    filtered = labels.view(-1) != ignore_index\n",
        "\n",
        "    # Calculate focal loss\n",
        "    ce_loss_filtered = ce_loss[filtered]\n",
        "    pred_probs_filtered = pred_probs[filtered]\n",
        "    pred_class_filtered = pred_class[filtered]\n",
        "\n",
        "    # Construct the loss\n",
        "    pt = pred_probs_filtered.gather(1, pred_class_filtered.unsqueeze(-1)).squeeze()\n",
        "    loss = ((1 - pt) ** gamma * ce_loss_filtered).mean()  # mean over the batch\n",
        "\n",
        "    return loss\n",
        "\n"
      ],
      "metadata": {
        "id": "_247Nj559I5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    tr_loss, tr_accuracy = 0, 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    tr_preds, tr_labels = [], []\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for idx, batch in enumerate(training_loader):\n",
        "        ids = batch['input_ids'].to(device, dtype=torch.long)\n",
        "        mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
        "        labels = batch['labels'].to(device, dtype=torch.long)\n",
        "\n",
        "        # Perform a forward pass to get the logits\n",
        "        outputs = model(input_ids=ids, attention_mask=mask)\n",
        "\n",
        "        # Calculate the loss using focal_loss function directly\n",
        "        loss = focal_loss(outputs.logits, labels)  # Use the focal_loss function here directly\n",
        "\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples += labels.size(0)\n",
        "\n",
        "        if idx % 100 == 0:\n",
        "            loss_step = tr_loss / nb_tr_steps\n",
        "            print(f\"Training loss per 100 training steps: {loss_step}\")\n",
        "\n",
        "        # Decoding the logits to compute accuracy\n",
        "        active_logits = outputs.logits.view(-1, model.num_labels)\n",
        "        flattened_predictions = torch.argmax(active_logits, axis=1)\n",
        "\n",
        "        # Ignoring the predictions of the padding tokens\n",
        "        active_accuracy = labels.view(-1) != -100\n",
        "        active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n",
        "\n",
        "        labels = torch.masked_select(active_labels, active_accuracy)\n",
        "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "\n",
        "        tr_labels.extend(labels.cpu().numpy())\n",
        "        tr_preds.extend(predictions.cpu().numpy())\n",
        "\n",
        "        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
        "        tr_accuracy += tmp_tr_accuracy\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=MAX_GRAD_NORM)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = tr_loss / nb_tr_steps\n",
        "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
        "    print(f\"Training loss epoch: {epoch_loss}\")\n",
        "    print(f\"Training accuracy epoch: {tr_accuracy}\")\n"
      ],
      "metadata": {
        "id": "99XcALDnQk_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Training epoch: {epoch + 1}\")\n",
        "    train(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9yYcqE5Qn62",
        "outputId": "ff4457e5-4325-4f0b-b336-8e90b9f9169e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 2.842136859893799\n",
            "Training loss per 100 training steps: 0.734010686526204\n",
            "Training loss per 100 training steps: 0.5077567323046129\n",
            "Training loss per 100 training steps: 0.37988267410171384\n",
            "Training loss per 100 training steps: 0.3199064199967447\n",
            "Training loss per 100 training steps: 0.27990488253011947\n",
            "Training loss per 100 training steps: 0.2490979599130701\n",
            "Training loss per 100 training steps: 0.2310740789188057\n",
            "Training loss per 100 training steps: 0.2150942471891583\n",
            "Training loss per 100 training steps: 0.2005705900594489\n",
            "Training loss per 100 training steps: 0.18988018857310415\n",
            "Training loss per 100 training steps: 0.17966743444561406\n",
            "Training loss per 100 training steps: 0.16948954458144896\n",
            "Training loss per 100 training steps: 0.16175487525996912\n",
            "Training loss per 100 training steps: 0.15638745718278915\n",
            "Training loss per 100 training steps: 0.15151218879712042\n",
            "Training loss per 100 training steps: 0.14506588451340116\n",
            "Training loss per 100 training steps: 0.14021930791471282\n",
            "Training loss per 100 training steps: 0.1354438495034993\n",
            "Training loss per 100 training steps: 0.1324461021522582\n",
            "Training loss per 100 training steps: 0.12974079669450178\n",
            "Training loss per 100 training steps: 0.12584802282602103\n",
            "Training loss per 100 training steps: 0.12291005196984464\n",
            "Training loss per 100 training steps: 0.11973247773285367\n",
            "Training loss per 100 training steps: 0.11717987020850631\n",
            "Training loss per 100 training steps: 0.11430863223157729\n",
            "Training loss per 100 training steps: 0.112222153907358\n",
            "Training loss per 100 training steps: 0.11004033523640128\n",
            "Training loss per 100 training steps: 0.10786624916313113\n",
            "Training loss per 100 training steps: 0.1053664834714453\n",
            "Training loss per 100 training steps: 0.10420466062402055\n",
            "Training loss per 100 training steps: 0.10250506102365955\n",
            "Training loss per 100 training steps: 0.10074541754182963\n",
            "Training loss per 100 training steps: 0.09919097974962432\n",
            "Training loss per 100 training steps: 0.09757070989193259\n",
            "Training loss per 100 training steps: 0.0964955580662077\n",
            "Training loss per 100 training steps: 0.09489740285413167\n",
            "Training loss per 100 training steps: 0.0936288353929377\n",
            "Training loss per 100 training steps: 0.0923990012412575\n",
            "Training loss per 100 training steps: 0.09093564760196673\n",
            "Training loss per 100 training steps: 0.08977725606788307\n",
            "Training loss per 100 training steps: 0.0892333726155682\n",
            "Training loss per 100 training steps: 0.08830162268601817\n",
            "Training loss per 100 training steps: 0.08719114759006485\n",
            "Training loss per 100 training steps: 0.08623019881068397\n",
            "Training loss per 100 training steps: 0.08503176723925453\n",
            "Training loss per 100 training steps: 0.08437833877689437\n",
            "Training loss per 100 training steps: 0.08366762589238222\n",
            "Training loss per 100 training steps: 0.08298619551044631\n",
            "Training loss per 100 training steps: 0.08189718838294795\n",
            "Training loss per 100 training steps: 0.08095827375268567\n",
            "Training loss per 100 training steps: 0.08025032080902778\n",
            "Training loss per 100 training steps: 0.07944140732802783\n",
            "Training loss per 100 training steps: 0.07863060516791844\n",
            "Training loss per 100 training steps: 0.07779329239096808\n",
            "Training loss per 100 training steps: 0.0772257183014815\n",
            "Training loss per 100 training steps: 0.07680493514939772\n",
            "Training loss per 100 training steps: 0.07615259797398581\n",
            "Training loss per 100 training steps: 0.075570899891257\n",
            "Training loss per 100 training steps: 0.07491266273390702\n",
            "Training loss per 100 training steps: 0.07433882138415734\n",
            "Training loss per 100 training steps: 0.07389413333000303\n",
            "Training loss per 100 training steps: 0.07322987445694473\n",
            "Training loss per 100 training steps: 0.07271830675443246\n",
            "Training loss per 100 training steps: 0.07221912762125336\n",
            "Training loss per 100 training steps: 0.07175473312031042\n",
            "Training loss per 100 training steps: 0.07132258395820756\n",
            "Training loss per 100 training steps: 0.0707216621668595\n",
            "Training loss per 100 training steps: 0.07011901738371018\n",
            "Training loss per 100 training steps: 0.06969038542283768\n",
            "Training loss per 100 training steps: 0.06926983316556033\n",
            "Training loss per 100 training steps: 0.06875007535804573\n",
            "Training loss per 100 training steps: 0.0682830027155421\n",
            "Training loss per 100 training steps: 0.06792892840849095\n",
            "Training loss per 100 training steps: 0.06756245358260739\n",
            "Training loss per 100 training steps: 0.06718252763526757\n",
            "Training loss per 100 training steps: 0.06690836158105773\n",
            "Training loss per 100 training steps: 0.06664510614976409\n",
            "Training loss per 100 training steps: 0.06638733314376219\n",
            "Training loss per 100 training steps: 0.06594108085114558\n",
            "Training loss per 100 training steps: 0.06566223648171292\n",
            "Training loss per 100 training steps: 0.06516976032668043\n",
            "Training loss per 100 training steps: 0.06498688184726212\n",
            "Training loss per 100 training steps: 0.06458583669965638\n",
            "Training loss per 100 training steps: 0.0641951562580864\n",
            "Training loss per 100 training steps: 0.06371986626245824\n",
            "Training loss per 100 training steps: 0.06331557126723779\n",
            "Training loss per 100 training steps: 0.06291089174971105\n",
            "Training loss per 100 training steps: 0.06258400693966337\n",
            "Training loss per 100 training steps: 0.06225901926538159\n",
            "Training loss per 100 training steps: 0.06200261008197679\n",
            "Training loss per 100 training steps: 0.0616555169183094\n",
            "Training loss per 100 training steps: 0.061291443781235196\n",
            "Training loss per 100 training steps: 0.06100890917934163\n",
            "Training loss per 100 training steps: 0.0607244476571338\n",
            "Training loss per 100 training steps: 0.060377985488459456\n",
            "Training loss per 100 training steps: 0.060025644118367256\n",
            "Training loss per 100 training steps: 0.05969194215448358\n",
            "Training loss per 100 training steps: 0.05936538188652649\n",
            "Training loss per 100 training steps: 0.05910898025932951\n",
            "Training loss per 100 training steps: 0.05889359596725285\n",
            "Training loss per 100 training steps: 0.05869576033030186\n",
            "Training loss per 100 training steps: 0.05844936335715104\n",
            "Training loss per 100 training steps: 0.058065196014387546\n",
            "Training loss per 100 training steps: 0.05791063894183109\n",
            "Training loss per 100 training steps: 0.05762088866219131\n",
            "Training loss per 100 training steps: 0.05743385391114202\n",
            "Training loss per 100 training steps: 0.05719504047191558\n",
            "Training loss per 100 training steps: 0.057001967460482694\n",
            "Training loss per 100 training steps: 0.05669615935504959\n",
            "Training loss per 100 training steps: 0.0564799810875161\n",
            "Training loss per 100 training steps: 0.056337450427880326\n",
            "Training loss per 100 training steps: 0.05612947956987616\n",
            "Training loss per 100 training steps: 0.05598853390571951\n",
            "Training loss per 100 training steps: 0.05571564157335648\n",
            "Training loss per 100 training steps: 0.055581536130430396\n",
            "Training loss per 100 training steps: 0.05542057891417038\n",
            "Training loss per 100 training steps: 0.05524488174034819\n",
            "Training loss per 100 training steps: 0.054997368155692124\n",
            "Training loss per 100 training steps: 0.054788828491719274\n",
            "Training loss per 100 training steps: 0.05456824297099275\n",
            "Training loss per 100 training steps: 0.05446130500026378\n",
            "Training loss per 100 training steps: 0.054298323578284485\n",
            "Training loss per 100 training steps: 0.05413469050594888\n",
            "Training loss per 100 training steps: 0.05395261895506491\n",
            "Training loss per 100 training steps: 0.05379220005990453\n",
            "Training loss per 100 training steps: 0.053613636627394164\n",
            "Training loss per 100 training steps: 0.05350022040914481\n",
            "Training loss per 100 training steps: 0.05333051837436351\n",
            "Training loss per 100 training steps: 0.053240064280833006\n",
            "Training loss per 100 training steps: 0.0530993651838988\n",
            "Training loss per 100 training steps: 0.052932469766126915\n",
            "Training loss per 100 training steps: 0.05277128432462952\n",
            "Training loss per 100 training steps: 0.05263457884903728\n",
            "Training loss epoch: 0.05241975795119763\n",
            "Training accuracy epoch: 0.9738645066309459\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.00023424580285791308\n",
            "Training loss per 100 training steps: 0.01756185650511986\n",
            "Training loss per 100 training steps: 0.017865116309608377\n",
            "Training loss per 100 training steps: 0.022386646183100233\n",
            "Training loss per 100 training steps: 0.024496676578104386\n",
            "Training loss per 100 training steps: 0.023408174076493905\n",
            "Training loss per 100 training steps: 0.023598370791198715\n",
            "Training loss per 100 training steps: 0.023339890476881467\n",
            "Training loss per 100 training steps: 0.023332984241640816\n",
            "Training loss per 100 training steps: 0.022877921301416235\n",
            "Training loss per 100 training steps: 0.02317617886294036\n",
            "Training loss per 100 training steps: 0.023851829028598275\n",
            "Training loss per 100 training steps: 0.023431052104056913\n",
            "Training loss per 100 training steps: 0.023481549773065834\n",
            "Training loss per 100 training steps: 0.023680215984093946\n",
            "Training loss per 100 training steps: 0.02360478917557021\n",
            "Training loss per 100 training steps: 0.023565409435721347\n",
            "Training loss per 100 training steps: 0.023249699441347804\n",
            "Training loss per 100 training steps: 0.02277397485031285\n",
            "Training loss per 100 training steps: 0.02311819054501035\n",
            "Training loss per 100 training steps: 0.02337399657481178\n",
            "Training loss per 100 training steps: 0.023482155788356263\n",
            "Training loss per 100 training steps: 0.023809650895535415\n",
            "Training loss per 100 training steps: 0.0239096943462657\n",
            "Training loss per 100 training steps: 0.02387234541593458\n",
            "Training loss per 100 training steps: 0.02343006506160808\n",
            "Training loss per 100 training steps: 0.0232713334465405\n",
            "Training loss per 100 training steps: 0.023211931037297715\n",
            "Training loss per 100 training steps: 0.023224939333187054\n",
            "Training loss per 100 training steps: 0.02299192134826459\n",
            "Training loss per 100 training steps: 0.02319714629284584\n",
            "Training loss per 100 training steps: 0.023265486263970778\n",
            "Training loss per 100 training steps: 0.023299042634079906\n",
            "Training loss per 100 training steps: 0.023449632747423062\n",
            "Training loss per 100 training steps: 0.02363768255080651\n",
            "Training loss per 100 training steps: 0.023730561838716486\n",
            "Training loss per 100 training steps: 0.023579109487640464\n",
            "Training loss per 100 training steps: 0.02387821271699563\n",
            "Training loss per 100 training steps: 0.02373039549334105\n",
            "Training loss per 100 training steps: 0.02365245945996234\n",
            "Training loss per 100 training steps: 0.024026958776290117\n",
            "Training loss per 100 training steps: 0.02398475602020801\n",
            "Training loss per 100 training steps: 0.023983576504206423\n",
            "Training loss per 100 training steps: 0.02393691523048653\n",
            "Training loss per 100 training steps: 0.02384567412354603\n",
            "Training loss per 100 training steps: 0.023730243875625757\n",
            "Training loss per 100 training steps: 0.023802930101880747\n",
            "Training loss per 100 training steps: 0.024028215908323117\n",
            "Training loss per 100 training steps: 0.024007082883773557\n",
            "Training loss per 100 training steps: 0.024009402179184305\n",
            "Training loss per 100 training steps: 0.023895098744931346\n",
            "Training loss per 100 training steps: 0.023856924286150123\n",
            "Training loss per 100 training steps: 0.02373283590828512\n",
            "Training loss per 100 training steps: 0.02389020637766858\n",
            "Training loss per 100 training steps: 0.023820745638577067\n",
            "Training loss per 100 training steps: 0.023910452458176943\n",
            "Training loss per 100 training steps: 0.023927674498188823\n",
            "Training loss per 100 training steps: 0.023839104600495757\n",
            "Training loss per 100 training steps: 0.023754387571233413\n",
            "Training loss per 100 training steps: 0.024028481338163498\n",
            "Training loss per 100 training steps: 0.024086637294004766\n",
            "Training loss per 100 training steps: 0.02415587799965202\n",
            "Training loss per 100 training steps: 0.024254003352845686\n",
            "Training loss per 100 training steps: 0.024225126588558306\n",
            "Training loss per 100 training steps: 0.024224657871366736\n",
            "Training loss per 100 training steps: 0.024187059062449077\n",
            "Training loss per 100 training steps: 0.02416674866529967\n",
            "Training loss per 100 training steps: 0.024327789421825063\n",
            "Training loss per 100 training steps: 0.024378401125818537\n",
            "Training loss per 100 training steps: 0.024288959168069335\n",
            "Training loss per 100 training steps: 0.024155997950147563\n",
            "Training loss per 100 training steps: 0.024083579326455395\n",
            "Training loss per 100 training steps: 0.024022378546397223\n",
            "Training loss per 100 training steps: 0.024053463132030672\n",
            "Training loss per 100 training steps: 0.024083492669006966\n",
            "Training loss per 100 training steps: 0.024024355930649038\n",
            "Training loss per 100 training steps: 0.024017392837786963\n",
            "Training loss per 100 training steps: 0.023887785077195408\n",
            "Training loss per 100 training steps: 0.023977887897934626\n",
            "Training loss per 100 training steps: 0.023971222347085794\n",
            "Training loss per 100 training steps: 0.023875486707223514\n",
            "Training loss per 100 training steps: 0.02391789241088037\n",
            "Training loss per 100 training steps: 0.023933681136848347\n",
            "Training loss per 100 training steps: 0.0238960645352879\n",
            "Training loss per 100 training steps: 0.02390566675063826\n",
            "Training loss per 100 training steps: 0.02390654995618668\n",
            "Training loss per 100 training steps: 0.023885913723719707\n",
            "Training loss per 100 training steps: 0.023924390600451956\n",
            "Training loss per 100 training steps: 0.023804268420192568\n",
            "Training loss per 100 training steps: 0.023787371485335218\n",
            "Training loss per 100 training steps: 0.023718944756035972\n",
            "Training loss per 100 training steps: 0.023727609736191514\n",
            "Training loss per 100 training steps: 0.023815893624814403\n",
            "Training loss per 100 training steps: 0.023732461298052072\n",
            "Training loss per 100 training steps: 0.023696026776570536\n",
            "Training loss per 100 training steps: 0.02372600116908573\n",
            "Training loss per 100 training steps: 0.023680217196286\n",
            "Training loss per 100 training steps: 0.02363213665825402\n",
            "Training loss per 100 training steps: 0.02358157426647404\n",
            "Training loss per 100 training steps: 0.02365343012607043\n",
            "Training loss per 100 training steps: 0.02360088332804617\n",
            "Training loss per 100 training steps: 0.02363377390295947\n",
            "Training loss per 100 training steps: 0.023577335556371988\n",
            "Training loss per 100 training steps: 0.023544717233232076\n",
            "Training loss per 100 training steps: 0.023508357043835845\n",
            "Training loss per 100 training steps: 0.023500339145949584\n",
            "Training loss per 100 training steps: 0.023428011064544788\n",
            "Training loss per 100 training steps: 0.023393903185039484\n",
            "Training loss per 100 training steps: 0.023424988299908106\n",
            "Training loss per 100 training steps: 0.02336881951566251\n",
            "Training loss per 100 training steps: 0.023331940719844745\n",
            "Training loss per 100 training steps: 0.023304018417739963\n",
            "Training loss per 100 training steps: 0.023302481409940845\n",
            "Training loss per 100 training steps: 0.02329283881774788\n",
            "Training loss per 100 training steps: 0.02325528545707211\n",
            "Training loss per 100 training steps: 0.02330164082933471\n",
            "Training loss per 100 training steps: 0.0233662155079049\n",
            "Training loss per 100 training steps: 0.023332728679477447\n",
            "Training loss per 100 training steps: 0.023328572860230594\n",
            "Training loss per 100 training steps: 0.0233196816744988\n",
            "Training loss per 100 training steps: 0.02337137835773418\n",
            "Training loss per 100 training steps: 0.02333110406303035\n",
            "Training loss per 100 training steps: 0.023362355509211244\n",
            "Training loss per 100 training steps: 0.023455360353088657\n",
            "Training loss per 100 training steps: 0.023460673487400302\n",
            "Training loss per 100 training steps: 0.023483325218455607\n",
            "Training loss per 100 training steps: 0.02346400076551763\n",
            "Training loss per 100 training steps: 0.02343606764730854\n",
            "Training loss per 100 training steps: 0.023399378154975628\n",
            "Training loss per 100 training steps: 0.023385094988285227\n",
            "Training loss per 100 training steps: 0.023365646862966664\n",
            "Training loss per 100 training steps: 0.023315340372543836\n",
            "Training loss per 100 training steps: 0.023285889922732228\n",
            "Training loss per 100 training steps: 0.02327926674164239\n",
            "Training loss epoch: 0.023320188841817725\n",
            "Training accuracy epoch: 0.9821116700421216\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.003395750420168042\n",
            "Training loss per 100 training steps: 0.018572849188875817\n",
            "Training loss per 100 training steps: 0.014432354718753879\n",
            "Training loss per 100 training steps: 0.014437295005084335\n",
            "Training loss per 100 training steps: 0.014296597381446572\n",
            "Training loss per 100 training steps: 0.01359948215533219\n",
            "Training loss per 100 training steps: 0.013150476230359691\n",
            "Training loss per 100 training steps: 0.013234318533584276\n",
            "Training loss per 100 training steps: 0.013301329713619898\n",
            "Training loss per 100 training steps: 0.013534410831836792\n",
            "Training loss per 100 training steps: 0.013469295201388265\n",
            "Training loss per 100 training steps: 0.013475035570261548\n",
            "Training loss per 100 training steps: 0.013758244544458445\n",
            "Training loss per 100 training steps: 0.01397643281047403\n",
            "Training loss per 100 training steps: 0.014039192512149842\n",
            "Training loss per 100 training steps: 0.014249332886779596\n",
            "Training loss per 100 training steps: 0.014212008567192425\n",
            "Training loss per 100 training steps: 0.014085750113119919\n",
            "Training loss per 100 training steps: 0.014456684801727184\n",
            "Training loss per 100 training steps: 0.01436642028266937\n",
            "Training loss per 100 training steps: 0.014265235759130574\n",
            "Training loss per 100 training steps: 0.01420731279236075\n",
            "Training loss per 100 training steps: 0.014309753609192401\n",
            "Training loss per 100 training steps: 0.014365801465232699\n",
            "Training loss per 100 training steps: 0.014491306176576658\n",
            "Training loss per 100 training steps: 0.014368541394712277\n",
            "Training loss per 100 training steps: 0.014360310352899612\n",
            "Training loss per 100 training steps: 0.014591338943607897\n",
            "Training loss per 100 training steps: 0.014534931267247734\n",
            "Training loss per 100 training steps: 0.014656622065399203\n",
            "Training loss per 100 training steps: 0.015027985807600692\n",
            "Training loss per 100 training steps: 0.015279580498396282\n",
            "Training loss per 100 training steps: 0.015314702343717856\n",
            "Training loss per 100 training steps: 0.01523024099334441\n",
            "Training loss per 100 training steps: 0.015158472584899937\n",
            "Training loss per 100 training steps: 0.015245468131824879\n",
            "Training loss per 100 training steps: 0.015230153451511859\n",
            "Training loss per 100 training steps: 0.015194571012040132\n",
            "Training loss per 100 training steps: 0.01512115141718307\n",
            "Training loss per 100 training steps: 0.015075234021000927\n",
            "Training loss per 100 training steps: 0.014992503406270733\n",
            "Training loss per 100 training steps: 0.014925844754979068\n",
            "Training loss per 100 training steps: 0.014933254003234189\n",
            "Training loss per 100 training steps: 0.014969621554683234\n",
            "Training loss per 100 training steps: 0.014963326202190215\n",
            "Training loss per 100 training steps: 0.015063787260553364\n",
            "Training loss per 100 training steps: 0.015072721737339631\n",
            "Training loss per 100 training steps: 0.015052898425132813\n",
            "Training loss per 100 training steps: 0.015067200265931472\n",
            "Training loss per 100 training steps: 0.01512780893281814\n",
            "Training loss per 100 training steps: 0.015106028494629484\n",
            "Training loss per 100 training steps: 0.015107155709584992\n",
            "Training loss per 100 training steps: 0.015145075848844626\n",
            "Training loss per 100 training steps: 0.015340263581354886\n",
            "Training loss per 100 training steps: 0.015319525700991347\n",
            "Training loss per 100 training steps: 0.015272894692584997\n",
            "Training loss per 100 training steps: 0.015244101519688632\n",
            "Training loss per 100 training steps: 0.01531684140997789\n",
            "Training loss per 100 training steps: 0.015344777768357121\n",
            "Training loss per 100 training steps: 0.015315059043845431\n",
            "Training loss per 100 training steps: 0.015342611629054474\n",
            "Training loss per 100 training steps: 0.015343984254415665\n",
            "Training loss per 100 training steps: 0.015238577776935356\n",
            "Training loss per 100 training steps: 0.01527597347579701\n",
            "Training loss per 100 training steps: 0.015233700192761096\n",
            "Training loss per 100 training steps: 0.015209045977636285\n",
            "Training loss per 100 training steps: 0.015209387933837621\n",
            "Training loss per 100 training steps: 0.015183163448559347\n",
            "Training loss per 100 training steps: 0.015266726692833207\n",
            "Training loss per 100 training steps: 0.015280336644348493\n",
            "Training loss per 100 training steps: 0.015208292361870416\n",
            "Training loss per 100 training steps: 0.01519829762059837\n",
            "Training loss per 100 training steps: 0.015209499962137392\n",
            "Training loss per 100 training steps: 0.015202714857692038\n",
            "Training loss per 100 training steps: 0.015181758341320187\n",
            "Training loss per 100 training steps: 0.015231278834167802\n",
            "Training loss per 100 training steps: 0.015236080491895372\n",
            "Training loss per 100 training steps: 0.015223274665256206\n",
            "Training loss per 100 training steps: 0.015315925857648542\n",
            "Training loss per 100 training steps: 0.015389649776598319\n",
            "Training loss per 100 training steps: 0.015483310640309542\n",
            "Training loss per 100 training steps: 0.015547000242042685\n",
            "Training loss per 100 training steps: 0.015544108826684326\n",
            "Training loss per 100 training steps: 0.015551543624404604\n",
            "Training loss per 100 training steps: 0.015577814983670024\n",
            "Training loss per 100 training steps: 0.015647639177522567\n",
            "Training loss per 100 training steps: 0.015660226065692298\n",
            "Training loss per 100 training steps: 0.015588863991082696\n",
            "Training loss per 100 training steps: 0.01556488084035813\n",
            "Training loss per 100 training steps: 0.015596756332448407\n",
            "Training loss per 100 training steps: 0.015560736695949588\n",
            "Training loss per 100 training steps: 0.015566124946009813\n",
            "Training loss per 100 training steps: 0.015635800119606507\n",
            "Training loss per 100 training steps: 0.01558335978729604\n",
            "Training loss per 100 training steps: 0.015588905374398773\n",
            "Training loss per 100 training steps: 0.015768062879496194\n",
            "Training loss per 100 training steps: 0.01569756827411051\n",
            "Training loss per 100 training steps: 0.015661165389433997\n",
            "Training loss per 100 training steps: 0.01572218549484067\n",
            "Training loss per 100 training steps: 0.015690755879597073\n",
            "Training loss per 100 training steps: 0.015730945251081974\n",
            "Training loss per 100 training steps: 0.01570739490675213\n",
            "Training loss per 100 training steps: 0.015751168120206216\n",
            "Training loss per 100 training steps: 0.015718793493892772\n",
            "Training loss per 100 training steps: 0.01570375030919952\n",
            "Training loss per 100 training steps: 0.015701358252643554\n",
            "Training loss per 100 training steps: 0.01567128977406747\n",
            "Training loss per 100 training steps: 0.01571134355643461\n",
            "Training loss per 100 training steps: 0.015707608931524404\n",
            "Training loss per 100 training steps: 0.015661697203736558\n",
            "Training loss per 100 training steps: 0.01571690905872279\n",
            "Training loss per 100 training steps: 0.015769091255416087\n",
            "Training loss per 100 training steps: 0.015749481071419703\n",
            "Training loss per 100 training steps: 0.015737989756378024\n",
            "Training loss per 100 training steps: 0.015718103472240223\n",
            "Training loss per 100 training steps: 0.015681501614615102\n",
            "Training loss per 100 training steps: 0.01566897184379781\n",
            "Training loss per 100 training steps: 0.015722179599807436\n",
            "Training loss per 100 training steps: 0.01575428882131885\n",
            "Training loss per 100 training steps: 0.01576935094914842\n",
            "Training loss per 100 training steps: 0.015740762887286777\n",
            "Training loss per 100 training steps: 0.015760924058659115\n",
            "Training loss per 100 training steps: 0.0157301016779038\n",
            "Training loss per 100 training steps: 0.015737961687398525\n",
            "Training loss per 100 training steps: 0.015724961372496318\n",
            "Training loss per 100 training steps: 0.015817167258695773\n",
            "Training loss per 100 training steps: 0.015815383742969362\n",
            "Training loss per 100 training steps: 0.015853531962907803\n",
            "Training loss per 100 training steps: 0.01593984086688079\n",
            "Training loss per 100 training steps: 0.01593292276090092\n",
            "Training loss per 100 training steps: 0.01590502206104148\n",
            "Training loss per 100 training steps: 0.015907750824189583\n",
            "Training loss per 100 training steps: 0.0159068737516998\n",
            "Training loss per 100 training steps: 0.015905927646099164\n",
            "Training loss epoch: 0.01588153783468314\n",
            "Training accuracy epoch: 0.9852481073654231\n",
            "Training epoch: 4\n",
            "Training loss per 100 training steps: 0.019399261102080345\n",
            "Training loss per 100 training steps: 0.008467604040050099\n",
            "Training loss per 100 training steps: 0.010380384947678404\n",
            "Training loss per 100 training steps: 0.011478108779365614\n",
            "Training loss per 100 training steps: 0.011268759583264428\n",
            "Training loss per 100 training steps: 0.01078769681796026\n",
            "Training loss per 100 training steps: 0.011094736725905255\n",
            "Training loss per 100 training steps: 0.011401331913390282\n",
            "Training loss per 100 training steps: 0.011091928894495638\n",
            "Training loss per 100 training steps: 0.010951451261091262\n",
            "Training loss per 100 training steps: 0.010916204715607484\n",
            "Training loss per 100 training steps: 0.010966566088335909\n",
            "Training loss per 100 training steps: 0.010478414467105977\n",
            "Training loss per 100 training steps: 0.010437956800854556\n",
            "Training loss per 100 training steps: 0.010667350773676327\n",
            "Training loss per 100 training steps: 0.010653159035324272\n",
            "Training loss per 100 training steps: 0.010857565208945801\n",
            "Training loss per 100 training steps: 0.010740536762012004\n",
            "Training loss per 100 training steps: 0.010738093430797168\n",
            "Training loss per 100 training steps: 0.010568543396300829\n",
            "Training loss per 100 training steps: 0.010573666300613724\n",
            "Training loss per 100 training steps: 0.010449709072609517\n",
            "Training loss per 100 training steps: 0.010257057863462277\n",
            "Training loss per 100 training steps: 0.010341082665172868\n",
            "Training loss per 100 training steps: 0.010328207584868462\n",
            "Training loss per 100 training steps: 0.010377730216262393\n",
            "Training loss per 100 training steps: 0.010319745684427619\n",
            "Training loss per 100 training steps: 0.010320629245200599\n",
            "Training loss per 100 training steps: 0.010369166616717336\n",
            "Training loss per 100 training steps: 0.010449360651680553\n",
            "Training loss per 100 training steps: 0.010503751480184297\n",
            "Training loss per 100 training steps: 0.010526145662956211\n",
            "Training loss per 100 training steps: 0.01051106534705423\n",
            "Training loss per 100 training steps: 0.010537994328761697\n",
            "Training loss per 100 training steps: 0.010449642616066604\n",
            "Training loss per 100 training steps: 0.010454882297581654\n",
            "Training loss per 100 training steps: 0.010521066700553616\n",
            "Training loss per 100 training steps: 0.01035146296803009\n",
            "Training loss per 100 training steps: 0.010479438726119922\n",
            "Training loss per 100 training steps: 0.010678770463758255\n",
            "Training loss per 100 training steps: 0.010627270682263532\n",
            "Training loss per 100 training steps: 0.010677328812407733\n",
            "Training loss per 100 training steps: 0.010633070220913487\n",
            "Training loss per 100 training steps: 0.010668348257202104\n",
            "Training loss per 100 training steps: 0.010769665517058278\n",
            "Training loss per 100 training steps: 0.010695142769467867\n",
            "Training loss per 100 training steps: 0.010667907756556993\n",
            "Training loss per 100 training steps: 0.010615919730768116\n",
            "Training loss per 100 training steps: 0.010759080402467256\n",
            "Training loss per 100 training steps: 0.010872452121678925\n",
            "Training loss per 100 training steps: 0.010810596411424992\n",
            "Training loss per 100 training steps: 0.010836628868910103\n",
            "Training loss per 100 training steps: 0.01084633531530773\n",
            "Training loss per 100 training steps: 0.010754600632358245\n",
            "Training loss per 100 training steps: 0.010843867952951667\n",
            "Training loss per 100 training steps: 0.010884047365496712\n",
            "Training loss per 100 training steps: 0.010909835449788996\n",
            "Training loss per 100 training steps: 0.010858468073529885\n",
            "Training loss per 100 training steps: 0.010801890999383348\n",
            "Training loss per 100 training steps: 0.010774646618709862\n",
            "Training loss per 100 training steps: 0.010789601734244163\n",
            "Training loss per 100 training steps: 0.010778350081107526\n",
            "Training loss per 100 training steps: 0.010731373679723029\n",
            "Training loss per 100 training steps: 0.010747461824607613\n",
            "Training loss per 100 training steps: 0.010697875918968482\n",
            "Training loss per 100 training steps: 0.010747367773375765\n",
            "Training loss per 100 training steps: 0.010724801697985342\n",
            "Training loss per 100 training steps: 0.010708606465224993\n",
            "Training loss per 100 training steps: 0.010708234831477044\n",
            "Training loss per 100 training steps: 0.010728488325407631\n",
            "Training loss per 100 training steps: 0.010708016730348277\n",
            "Training loss per 100 training steps: 0.010708797159731795\n",
            "Training loss per 100 training steps: 0.010731332513717008\n",
            "Training loss per 100 training steps: 0.010729234450857756\n",
            "Training loss per 100 training steps: 0.010762291542866193\n",
            "Training loss per 100 training steps: 0.010733713190996655\n",
            "Training loss per 100 training steps: 0.01078971958097037\n",
            "Training loss per 100 training steps: 0.010773239852045787\n",
            "Training loss per 100 training steps: 0.010747247416127587\n",
            "Training loss per 100 training steps: 0.01074441210492778\n",
            "Training loss per 100 training steps: 0.01073950754812175\n",
            "Training loss per 100 training steps: 0.010747453726147835\n",
            "Training loss per 100 training steps: 0.01073019169787418\n",
            "Training loss per 100 training steps: 0.01078141177365028\n",
            "Training loss per 100 training steps: 0.010854783420170301\n",
            "Training loss per 100 training steps: 0.010814024989801593\n",
            "Training loss per 100 training steps: 0.010807496545919375\n",
            "Training loss per 100 training steps: 0.010757779165705113\n",
            "Training loss per 100 training steps: 0.010801594393222922\n",
            "Training loss per 100 training steps: 0.010789084095143908\n",
            "Training loss per 100 training steps: 0.010812491542538459\n",
            "Training loss per 100 training steps: 0.01081781688252387\n",
            "Training loss per 100 training steps: 0.010801295207796461\n",
            "Training loss per 100 training steps: 0.010791198299058577\n",
            "Training loss per 100 training steps: 0.01081070050056028\n",
            "Training loss per 100 training steps: 0.010781892030799156\n",
            "Training loss per 100 training steps: 0.010762168598131795\n",
            "Training loss per 100 training steps: 0.010787431030939317\n",
            "Training loss per 100 training steps: 0.010773883010813563\n",
            "Training loss per 100 training steps: 0.010758351754090425\n",
            "Training loss per 100 training steps: 0.010753287372929892\n",
            "Training loss per 100 training steps: 0.010734324203125074\n",
            "Training loss per 100 training steps: 0.010743081399508903\n",
            "Training loss per 100 training steps: 0.01075385039818698\n",
            "Training loss per 100 training steps: 0.010760190699111306\n",
            "Training loss per 100 training steps: 0.010788825885766273\n",
            "Training loss per 100 training steps: 0.010827918803980178\n",
            "Training loss per 100 training steps: 0.010867134722475982\n",
            "Training loss per 100 training steps: 0.010868300484989588\n",
            "Training loss per 100 training steps: 0.010854996849834158\n",
            "Training loss per 100 training steps: 0.010865627204510377\n",
            "Training loss per 100 training steps: 0.010867108385560513\n",
            "Training loss per 100 training steps: 0.010872974238404264\n",
            "Training loss per 100 training steps: 0.010869304817992618\n",
            "Training loss per 100 training steps: 0.010918222642569855\n",
            "Training loss per 100 training steps: 0.01092558820933679\n",
            "Training loss per 100 training steps: 0.01095906034558871\n",
            "Training loss per 100 training steps: 0.010941519525948119\n",
            "Training loss per 100 training steps: 0.010965723639739536\n",
            "Training loss per 100 training steps: 0.010967269568738556\n",
            "Training loss per 100 training steps: 0.01097781543589749\n",
            "Training loss per 100 training steps: 0.01098308888789256\n",
            "Training loss per 100 training steps: 0.01101422425596808\n",
            "Training loss per 100 training steps: 0.01103472687391987\n",
            "Training loss per 100 training steps: 0.01102481778171446\n",
            "Training loss per 100 training steps: 0.01102867333855916\n",
            "Training loss per 100 training steps: 0.0110345223533316\n",
            "Training loss per 100 training steps: 0.011028944760669526\n",
            "Training loss per 100 training steps: 0.011068690701536116\n",
            "Training loss per 100 training steps: 0.011062166936632872\n",
            "Training loss per 100 training steps: 0.011147392446984906\n",
            "Training loss per 100 training steps: 0.01115673902674583\n",
            "Training loss per 100 training steps: 0.011198616628322111\n",
            "Training loss per 100 training steps: 0.011181195634077304\n",
            "Training loss epoch: 0.011171140282961452\n",
            "Training accuracy epoch: 0.9887320075572033\n",
            "Training epoch: 5\n",
            "Training loss per 100 training steps: 1.0786897291836794e-05\n",
            "Training loss per 100 training steps: 0.006285142097909773\n",
            "Training loss per 100 training steps: 0.00730282301707297\n",
            "Training loss per 100 training steps: 0.007635505373602588\n",
            "Training loss per 100 training steps: 0.007988280730631587\n",
            "Training loss per 100 training steps: 0.008245845882260674\n",
            "Training loss per 100 training steps: 0.007966569227231997\n",
            "Training loss per 100 training steps: 0.007761522501417723\n",
            "Training loss per 100 training steps: 0.007671489114683522\n",
            "Training loss per 100 training steps: 0.007268334279041703\n",
            "Training loss per 100 training steps: 0.007183631019691469\n",
            "Training loss per 100 training steps: 0.007390257246056033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def valid(model, testing_loader):\n",
        "    model.eval()\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_examples, nb_eval_steps = 0, 0\n",
        "    eval_preds, eval_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(testing_loader):\n",
        "            ids = batch['input_ids'].to(device, dtype=torch.long)\n",
        "            mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
        "            labels = batch['labels'].to(device, dtype=torch.long)\n",
        "\n",
        "            outputs = model(input_ids=ids, attention_mask=mask)\n",
        "            loss = focal_loss(outputs.logits, labels)  # Use the same focal_loss function\n",
        "\n",
        "            eval_loss += loss.item()\n",
        "            nb_eval_steps += 1\n",
        "            nb_eval_examples += labels.size(0)\n",
        "\n",
        "            if idx % 100 == 0:\n",
        "                loss_step = eval_loss / nb_eval_steps\n",
        "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
        "\n",
        "            # Decoding the logits to compute accuracy\n",
        "            active_logits = outputs.logits.view(-1, model.num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1)\n",
        "\n",
        "            # Ignoring the predictions of the padding tokens\n",
        "            active_accuracy = labels.view(-1) != -100\n",
        "            active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n",
        "\n",
        "            labels = torch.masked_select(active_labels, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "\n",
        "            eval_labels.extend(labels.cpu().numpy())\n",
        "            eval_preds.extend(predictions.cpu().numpy())\n",
        "\n",
        "            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "    labels = [ids_to_labels[id.item()] for id in eval_labels]\n",
        "    predictions = [ids_to_labels[id.item()] for id in eval_preds]\n",
        "\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
        "    print(f\"Validation Loss: {eval_loss}\")\n",
        "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
        "\n",
        "    return labels, predictions\n"
      ],
      "metadata": {
        "id": "-yPKcQuJZR_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_eval(model, testing_loader):\n",
        "    # put model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_examples, nb_eval_steps = 0, 0\n",
        "    eval_preds, eval_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(testing_loader):\n",
        "\n",
        "            ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "            labels = batch['labels'].to(device, dtype = torch.long)\n",
        "\n",
        "            loss, eval_logits = model(input_ids=ids, attention_mask=mask, labels=labels, return_dict=False)\n",
        "\n",
        "            eval_loss += loss.item()\n",
        "\n",
        "            nb_eval_steps += 1\n",
        "            nb_eval_examples += labels.size(0)\n",
        "\n",
        "            if idx % 100==0:\n",
        "                loss_step = eval_loss/nb_eval_steps\n",
        "                print(f\"Test loss per 100 evaluation steps: {loss_step}\")\n",
        "\n",
        "            # compute evaluation accuracy\n",
        "            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "\n",
        "            # only compute accuracy at active labels\n",
        "            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
        "\n",
        "            labels = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "\n",
        "            eval_labels.extend(labels)\n",
        "            eval_preds.extend(predictions)\n",
        "\n",
        "            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "    labels = [ids_to_labels[id.item()] for id in eval_labels]\n",
        "    predictions = [ids_to_labels[id.item()] for id in eval_preds]\n",
        "\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
        "    print(f\"Test Loss: {eval_loss}\")\n",
        "    print(f\"Test Accuracy: {eval_accuracy}\")\n",
        "\n",
        "    return labels, predictions"
      ],
      "metadata": {
        "id": "PGIt_PbW4wOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels, predictions = valid(model, val_loader)\n"
      ],
      "metadata": {
        "id": "jsTZFFHJgHIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels, test_preds = valid(model, testing_loader)"
      ],
      "metadata": {
        "id": "gUcJIRmR5rlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BnywlzqosAE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval"
      ],
      "metadata": {
        "id": "JLeO1nEqr-au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import classification_report\n",
        "\n",
        "print(classification_report([labels], [predictions]))"
      ],
      "metadata": {
        "id": "UcthIoavzjlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report([test_labels], [test_preds]))"
      ],
      "metadata": {
        "id": "QqpaqDl26o7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "directory = \"/content/drive/MyDrive/266_final/model_clinicalbert_train_val_test_FL\"\n",
        "\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "# save vocabulary of the tokenizer\n",
        "tokenizer.save_vocabulary(directory)\n",
        "# save the model weights and its configuration file\n",
        "model.save_pretrained(directory)\n",
        "print('All files saved')\n",
        "print('This tutorial is completed')"
      ],
      "metadata": {
        "id": "SmhHNpaCzni9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDOInmcW3qwe"
      },
      "source": [
        "Resources\n",
        "\n",
        "### https://github.com/lcampillos/Medical-NER/blob/master/bert_ner.ipynb\n",
        "### https://medium.com/analytics-vidhya/bio-tagged-text-to-original-text-99b05da6664"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}