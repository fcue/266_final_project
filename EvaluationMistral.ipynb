{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOQ8e1ncnhQK",
        "outputId": "b11bbd93-38e8-428b-fc5c-597ad31d1e7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "IfNUbhNLnr_9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/266_project/mistral_7b_data/entity_predictions.csv')\n",
        "df2 = pd.read_csv('/content/drive/My Drive/266_project/mistral_7b_data/entity_predictions_100.csv')"
      ],
      "metadata": {
        "id": "5BR1Byxtnwsv"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows' ,None)\n",
        "pd.set_option('display.width', None)"
      ],
      "metadata": {
        "id": "TGQbOlwKn1wb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "-5d2DDm7n2mI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def compare_predictions(row):\n",
        "#     results = {\n",
        "#         'correct': [],\n",
        "#         'incorrect': []\n",
        "#     }\n",
        "\n",
        "#     if pd.notna(row['Prediction']):\n",
        "#         prediction = ast.literal_eval(row['Prediction'])  # Safer than eval\n",
        "#     else:\n",
        "#         prediction = {}  # Use an empty dictionary if prediction is NaN\n",
        "\n",
        "#     true_value = ast.literal_eval(row['True Value'])\n",
        "\n",
        "#     # keys to compare - union of both dictionaries' keys\n",
        "#     all_keys = set(prediction.keys()) | set(true_value.keys())\n",
        "\n",
        "#     for key in all_keys:\n",
        "#         pred_val = prediction.get(key, [])  # Defaults to an empty list if key is not in prediction\n",
        "#         true_val = true_value.get(key, [])  # Defaults to an empty list if key is not in true_value\n",
        "\n",
        "#         if pred_val == true_val and pred_val != []:  # Correct prediction\n",
        "#             results['correct'].append(key)\n",
        "#         elif pred_val != true_val:  # Incorrect prediction\n",
        "#             results['incorrect'].append(key)\n",
        "\n",
        "#     return results\n",
        "\n",
        "\n",
        "# df['Comparison_Results'] = df.apply(compare_predictions, axis=1)\n",
        "\n",
        "# df[['Comparison_Results']]\n"
      ],
      "metadata": {
        "id": "eKdIJkMLoIgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['True Value'] = df['True Value'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else {})\n",
        "df['Prediction'] = df['Prediction'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else {})\n",
        "\n",
        "# fn for true pos, false pos, false neg\n",
        "def calculate_metrics(row):\n",
        "    true_entities = set(row['True Value'].get('Drug', []))\n",
        "    predicted_entities = set(row['Prediction'].get('Drug', []))\n",
        "\n",
        "    tp = len(true_entities & predicted_entities)  # Intersection gives True Positives\n",
        "    fp = len(predicted_entities - true_entities)   # Difference gives False Positives\n",
        "    fn = len(true_entities - predicted_entities)   # Difference gives False Negatives\n",
        "\n",
        "    return pd.Series([fp, fn, tp], index=['False Positives', 'False Negatives', 'True Positives'])\n",
        "\n",
        "df[['False Positives', 'False Negatives', 'True Positives']] = df.apply(calculate_metrics, axis=1)\n",
        "df2[['False Positives', 'False Negatives', 'True Positives']] = df2.apply(calculate_metrics, axis=1)"
      ],
      "metadata": {
        "id": "e3XCq1EhtD3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# fn to safely evaluate a string or return the input if it's already a dictionary\n",
        "def safe_eval(x):\n",
        "    if isinstance(x, str):\n",
        "        try:\n",
        "            return ast.literal_eval(x)\n",
        "        except (ValueError, SyntaxError):\n",
        "            return x\n",
        "    return x\n",
        "\n",
        "df['True Value'] = df['True Value'].apply(safe_eval)\n",
        "df['Prediction'] = df['Prediction'].apply(safe_eval)\n",
        "\n",
        "# calculate_metrics function\n",
        "def calculate_metrics(row):\n",
        "    fp_list, fn_list, tp_list = [], [], []\n",
        "    all_keys = set(row['True Value'].keys()) | set(row['Prediction'].keys())\n",
        "\n",
        "    for key in all_keys:\n",
        "        true_entities = set(row['True Value'].get(key, []))\n",
        "        predicted_entities = set(row['Prediction'].get(key, []))\n",
        "\n",
        "        tp = len(true_entities & predicted_entities)\n",
        "        fp = len(predicted_entities - true_entities)\n",
        "        fn = len(true_entities - predicted_entities)\n",
        "\n",
        "        fp_list.append(fp)\n",
        "        fn_list.append(fn)\n",
        "        tp_list.append(tp)\n",
        "\n",
        "    total_fp = sum(fp_list)\n",
        "    total_fn = sum(fn_list)\n",
        "    total_tp = sum(tp_list)\n",
        "\n",
        "    return pd.Series([total_fp, total_fn, total_tp], index=['False Positives', 'False Negatives', 'True Positives'])\n",
        "\n",
        "df[['False Positives', 'False Negatives', 'True Positives']] = df.apply(calculate_metrics, axis=1)\n",
        "df2[['False Positives', 'False Negatives', 'True Positives']] = df2.apply(calculate_metrics, axis=1)"
      ],
      "metadata": {
        "id": "Z2NtujMnva19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating precision, recall, and F1 score\n",
        "df['Precision'] = df['True Positives'] / (df['True Positives'] + df['False Positives'])\n",
        "df['Recall'] = df['True Positives'] / (df['True Positives'] + df['False Negatives'])\n",
        "df['F1 Score'] = 2 * (df['Precision'] * df['Recall']) / (df['Precision'] + df['Recall'])\n",
        "\n",
        "# Handling potential division by zero\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Calculate the averages for Precision, Recall, and F1 Score\n",
        "average_precision = df['Precision'].mean()\n",
        "average_recall = df['Recall'].mean()\n",
        "average_f1_score = df['F1 Score'].mean()\n",
        "\n",
        "average_precision, average_recall, average_f1_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLVgCmK8p_2H",
        "outputId": "7c25bbe6-6552-493f-c1a8-31a0e1ca5a29"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3353333333333333, 0.337, 0.33099999999999996)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered = df2[df2['Prediction'].apply(lambda x: bool(x))]"
      ],
      "metadata": {
        "id": "ucgtRe8j0MKi"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 =df_filtered.reset_index()"
      ],
      "metadata": {
        "id": "agHlS7_l2Hlp"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['Precision'] = df2['True Positives'] / (df2['True Positives'] + df2['False Positives'])\n",
        "df2['Recall'] = df2['True Positives'] / (df2['True Positives'] + df2['False Negatives'])\n",
        "df2['F1 Score'] = 2 * (df2['Precision'] * df2['Recall']) / (df2['Precision'] + df2['Recall'])\n",
        "\n",
        "df2.fillna(0, inplace=True)\n",
        "\n",
        "average_precision = df2['Precision'].mean()\n",
        "average_recall = df2['Recall'].mean()\n",
        "average_f1_score = df2['F1 Score'].mean()\n",
        "\n",
        "average_precision, average_recall, average_f1_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HpKlc1-yWdG",
        "outputId": "6d6097d2-413e-4faa-a858-2df152b93443"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6572139303482587, 0.6838308457711442, 0.659077986689927)"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    }
  ]
}